{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xJJWf_j7WAZ"
      },
      "source": [
        "# **Diagnóstico de Diabetes com Redes Neurais**\n",
        "\n",
        "Nesta atividade, vamos trabalhar com um problema aplicado de **classificação binária**: prever se uma pessoa possui ou não diabetes com base em um conjunto de variáveis clínicas.\n",
        "\n",
        "[Pima Indians Diabetes Database](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)\n",
        "\n",
        "---\n",
        "\n",
        "## **Contexto**\n",
        "\n",
        "- O dataset utilizado é o **Pima Indians Diabetes Dataset**, coletado originalmente pelo Instituto Nacional de Diabetes e Doenças Digestivas e Renais dos Estados Unidos.\n",
        "- Ele contém registros de mulheres com pelo menos 21 anos de idade da população Pima, um grupo étnico nativo norte-americano com alta incidência de diabetes tipo 2.\n",
        "\n",
        "## **Objetivo**\n",
        "\n",
        "O objetivo é treinar uma MLP para prever a presença de diabetes a partir de atributos fisiológicos e laboratoriais.\n",
        "\n",
        "## **Variáveis de entrada**\n",
        "\n",
        "Cada observação contém os seguintes atributos:\n",
        "\n",
        "1. **Pregnancies**, number of times pregnant: Variável discreta.\n",
        "2. **Glucose**, plasma glucose concentration after 2 hours in an oral glucose tolerance test: Variável contínua.\n",
        "3. **BloodPressure**, diastolic blood pressure, in mm Hg: Variável contínua.\n",
        "4. **SkinThickness**, triceps skin fold thickness, in mm: Variável contínua.\n",
        "5. **Insulin**, 2-hour serum insulin, in μU/mL: Variável contínua.\n",
        "6. **BMI**, body mass index, weight in kg/(height in m)²: Variável contínua.\n",
        "7. **DiabetesPedigreeFunction**, family history function: Variável contínua.\n",
        "8. **Age**, in years: : Variável discreta.\n",
        "\n",
        "## **Variáveis de saída (Target)**\n",
        "\n",
        "- **Outcome = 1**: Diabetic\n",
        "- **Outcome = 0**: Non-diabetic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2nnhd38P2H-"
      },
      "source": [
        "# **Arquitetura da Rede Neural e Procedimentos Adotados**\n",
        "\n",
        "---\n",
        "\n",
        "## **Arquitetura da rede**\n",
        "\n",
        "A estrutura da rede foi definida como:\n",
        "\n",
        "- **Entrada**: $8$ variáveis de entrada (padronizadas).\n",
        "- **1ª camada oculta**: $6$ neurônios com ativação $\\phi(z)$.\n",
        "- **2ª camada oculta**: $3$ neurônios com ativação $\\phi(z)$.\n",
        "- **Camada de saída**: $2$ neurônios com ativação **Softmax**, representando as probabilidades associadas a cada classe (saída codificada em one-hot)\n",
        "\n",
        "## **Funções de Ativação**\n",
        "\n",
        "- Nas **camadas ocultas**, utilizamos a função Rectified Linear Unit (**ReLU**):\n",
        "  $$\n",
        "  \\phi(z) = \\max(0, z),\n",
        "  $$\n",
        "  computacionalmente eficiente e ajuda a evitar o problema de saturação presente em funções como a sigmoide.\n",
        "\n",
        "- Na **camada de saída**, utilizamos a função **Softmax**:\n",
        "  $$\n",
        "  \\text{softmax}(z_j) = \\frac{e^{z_j}}{\\sum_{k} e^{z_k}}\n",
        "  $$\n",
        "\n",
        "## **Função de Custo**\n",
        "\n",
        "Como a saída está codificada em **one-hot**, adotamos a **cross-entropy categórica** como função de custo:\n",
        "$$\n",
        "\\mathcal{L}(y, \\hat{y}) = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^{2} y_{ij} \\log(\\hat{y}_{ij})\n",
        "$$\n",
        "\n",
        "## **Procedimento de Otimização**\n",
        "\n",
        "O treinamento foi realizado utilizando o algoritmo de **descida do gradiente clássica (batch)**:\n",
        "\n",
        "- Os gradientes foram computados por meio do algoritmo de **backpropagation**. Os pesos foram atualizados de forma simultânea com base no erro de todo o conjunto de treino, com taxa de aprendizado $\\eta$.\n",
        "\n",
        "## **Tratamento dos Dados**\n",
        "\n",
        "Antes do treinamento, os dados foram processados da seguinte forma:\n",
        "\n",
        "- **Remoção de entradas inválidas**, com valores zero biologicamente implausíveis.\n",
        "- **Padronização** das variáveis de entrada via z-score.\n",
        "- **Codificação one-hot** da target (binária).\n",
        "- **Divisão em conjuntos** de treino (70%), validação (15%) e teste (15%).\n",
        "\n",
        "## **Avaliação**\n",
        "\n",
        "Durante o treinamento, foram monitoradas:\n",
        "\n",
        "- A **função de perda** (cross-entropy) em treino e validação.\n",
        "- A **acurácia** em ambos os conjuntos.\n",
        "\n",
        "Após o treinamento, o modelo foi avaliado no **conjunto de teste** por meio de:\n",
        "- Matriz de confusão apresentando métricas de acurácia, precisão, recall e $F_1$-score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "XOv3d7Y5SuGX",
        "outputId": "bed480d7-6c23-40ba-ace7-3eb6b07dedae"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# @title MLP com ReLU nas camadas ocultas e Softmax na saída (2 camadas ocultas)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnx\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Criar grafo direcionado\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "# @title MLP com ReLU nas camadas ocultas e Softmax na saída (2 camadas ocultas)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Criar grafo direcionado\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Camadas conforme a descrição\n",
        "input_layer = [f'x_{i+1}' for i in range(8)]\n",
        "hidden_layer1 = [f'h^1_{i+1}' for i in range(6)]\n",
        "hidden_layer2 = [f'h^2_{i+1}' for i in range(3)]\n",
        "output_layer = ['{y}_0', '{y}_1']\n",
        "\n",
        "# Lista de camadas\n",
        "layers = [input_layer, hidden_layer1, hidden_layer2, output_layer]\n",
        "positions = {}\n",
        "labels = {}\n",
        "layer_dist = 2.0\n",
        "node_dist = 1.0\n",
        "\n",
        "# Posicionamento dos nós\n",
        "for i, layer in enumerate(layers):\n",
        "    for j, node in enumerate(layer):\n",
        "        G.add_node(node)\n",
        "        positions[node] = (i * layer_dist, -j * node_dist)\n",
        "        labels[node] = f\"${node}$\"\n",
        "\n",
        "# Conectar camadas\n",
        "def connect_layers(layer1, layer2):\n",
        "    for u in layer1:\n",
        "        for v in layer2:\n",
        "            G.add_edge(u, v)\n",
        "\n",
        "connect_layers(input_layer, hidden_layer1)\n",
        "connect_layers(hidden_layer1, hidden_layer2)\n",
        "connect_layers(hidden_layer2, output_layer)\n",
        "\n",
        "# Desenho\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw_networkx_nodes(G, positions, node_color='lightgray', node_size=1000)\n",
        "nx.draw_networkx_edges(G, positions, arrows=True, arrowstyle='-|>', width=1)\n",
        "nx.draw_networkx_labels(G, positions, labels, font_size=12)\n",
        "plt.title('Arquitetura: Entrada (8) → Oculta1 (6, ReLU) → Oculta2 (3, ReLU) → Saída (2, Softmax)', fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "fOe_WcdP4oYQ",
        "outputId": "9aeac68c-8240-4135-b3d0-9a1becdf935f"
      },
      "outputs": [],
      "source": [
        "# @title Importação dos dados\n",
        "import pandas as pd\n",
        "\n",
        "# URL do dataset\n",
        "url = \"https://raw.githubusercontent.com/pcbrom/perceptron-mlp-cnn/refs/heads/main/data/diabetes.csv\"\n",
        "\n",
        "# Carregar o dataset\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Verificar dimensões\n",
        "print(f\"Shape do dataset: {df.shape}\")\n",
        "\n",
        "# Visualizar as primeiras linhas\n",
        "display(df.head())\n",
        "\n",
        "# Verificar estatísticas básicas\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "oiwrzTmZ_XIU",
        "outputId": "1df3816a-a3cc-4f21-9f9a-892039f26673"
      },
      "outputs": [],
      "source": [
        "# @title Verificação de valores zero inválidos (biologicamente implausíveis)\n",
        "\n",
        "# Colunas que não devem conter zero\n",
        "cols_with_invalid_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "\n",
        "# Contagem de zeros por coluna\n",
        "invalid_zeros = (df[cols_with_invalid_zeros] == 0).sum()\n",
        "\n",
        "print(\"Contagem de valores igual a zero (potencialmente inválidos):\")\n",
        "display(invalid_zeros)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9215-O_Z_oqc",
        "outputId": "1f6504fb-90ad-4c8b-8969-46d1c93396ec"
      },
      "outputs": [],
      "source": [
        "# @title Remoção de linhas com valores zero inválidos\n",
        "\n",
        "# Colunas que não devem conter zero\n",
        "cols_with_invalid_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "\n",
        "# Filtrar apenas linhas com valores válidos (não-zero) nas colunas indicadas\n",
        "df_clean = df.copy()\n",
        "for col in cols_with_invalid_zeros:\n",
        "    df_clean = df_clean[df_clean[col] != 0]\n",
        "\n",
        "# Verificar nova dimensão do conjunto de dados\n",
        "print(f\"Shape após remoção: {df_clean.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcorYHCnACMU",
        "outputId": "3d278963-8456-494d-d566-2c6ab854c50d"
      },
      "outputs": [],
      "source": [
        "# @title Padronização antes da divisão e One-Hot Encoding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Separar variáveis explicativas e alvo\n",
        "X = df_clean.drop(columns='Outcome').values\n",
        "y = df_clean['Outcome'].values.reshape(-1, 1)  # necessário para o encoder\n",
        "\n",
        "# Padronizar (z-score)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# One-Hot Encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "# 70% treino, 30% temporário (validação + teste)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_scaled, y_encoded, test_size=0.30, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 15% validação, 15% teste\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# Verificações\n",
        "print(f\"Tamanho do conjunto de treino: {X_train.shape[0]}\")\n",
        "print(f\"Tamanho do conjunto de validação: {X_val.shape[0]}\")\n",
        "print(f\"Tamanho do conjunto de teste: {X_test.shape[0]}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGfUpmjdBrF5"
      },
      "outputs": [],
      "source": [
        "# Hiperparâmetros\n",
        "eta = 0.0001     # teste com outros valores (exercício)\n",
        "epochs = 1500    # teste com outros valores (exercício)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "axyvoJOfAXeg",
        "outputId": "ae878085-26b4-4d66-cf58-235df64a34d5"
      },
      "outputs": [],
      "source": [
        "# @title MLP com ReLU nas camadas ocultas e Softmax na saída (2 camadas ocultas) DG\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Arquitetura\n",
        "n_input = X_train.shape[1]\n",
        "n_hidden1 = 6\n",
        "n_hidden2 = 3\n",
        "n_output = 2  # saída one-hot\n",
        "\n",
        "# Funções de ativação\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def drelu(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    z_stable = z - np.max(z, axis=1, keepdims=True)  # estabilidade numérica\n",
        "    exp_z = np.exp(z_stable)\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(y_hat, y_true):\n",
        "    eps = 1e-10\n",
        "    return -np.mean(np.sum(y_true * np.log(y_hat + eps), axis=1))\n",
        "\n",
        "# Inicialização\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(n_input, n_hidden1)\n",
        "b1 = np.zeros((1, n_hidden1))\n",
        "W2 = np.random.randn(n_hidden1, n_hidden2)\n",
        "b2 = np.zeros((1, n_hidden2))\n",
        "W3 = np.random.randn(n_hidden2, n_output)\n",
        "b3 = np.zeros((1, n_output))\n",
        "\n",
        "# Histórico\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # --- Forward (treino)\n",
        "    Z1 = X_train @ W1 + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = A1 @ W2 + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = A2 @ W3 + b3\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        "    # --- Forward (validação)\n",
        "    Z1v = X_val @ W1 + b1\n",
        "    A1v = relu(Z1v)\n",
        "    Z2v = A1v @ W2 + b2\n",
        "    A2v = relu(Z2v)\n",
        "    Z3v = A2v @ W3 + b3\n",
        "    A3v = softmax(Z3v)\n",
        "\n",
        "    # --- Loss\n",
        "    train_loss = cross_entropy(A3, y_train)\n",
        "    val_loss = cross_entropy(A3v, y_val)\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append(val_loss)\n",
        "\n",
        "    # --- Acurácia\n",
        "    train_pred = np.argmax(A3, axis=1)\n",
        "    val_pred = np.argmax(A3v, axis=1)\n",
        "    train_acc = np.mean(train_pred == np.argmax(y_train, axis=1))\n",
        "    val_acc = np.mean(val_pred == np.argmax(y_val, axis=1))\n",
        "    train_acc_history.append(train_acc)\n",
        "    val_acc_history.append(val_acc)\n",
        "\n",
        "    # --- Backpropagation\n",
        "    dZ3 = (A3 - y_train)  # derivada da softmax + cross-entropy\n",
        "    dW3 = A2.T @ dZ3\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = (dZ3 @ W3.T) * drelu(Z2)\n",
        "    dW2 = A1.T @ dZ2\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    dZ1 = (dZ2 @ W2.T) * drelu(Z1)\n",
        "    dW1 = X_train.T @ dZ1 # usa todos os dados simultaneamente\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    # --- Atualização\n",
        "    W3 -= eta * dW3\n",
        "    b3 -= eta * db3\n",
        "    W2 -= eta * dW2\n",
        "    b2 -= eta * db2\n",
        "    W1 -= eta * dW1\n",
        "    b1 -= eta * db1\n",
        "\n",
        "    # Log ocasional\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# --- Gráficos\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_history, label='Train Loss')\n",
        "plt.plot(val_loss_history, label='Val Loss')\n",
        "plt.title('Evolução da Função de Perda (Cross-Entropy)')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Acurácia\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc_history, label='Train Acc')\n",
        "plt.plot(val_acc_history, label='Val Acc')\n",
        "plt.title('Evolução da Acurácia')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Forward no conjunto de teste\n",
        "Z1t = X_test @ W1 + b1\n",
        "A1t = relu(Z1t)\n",
        "Z2t = A1t @ W2 + b2\n",
        "A2t = relu(Z2t)\n",
        "Z3t = A2t @ W3 + b3\n",
        "A3t = softmax(Z3t)\n",
        "\n",
        "# Predição: classe com maior probabilidade\n",
        "y_pred_test = np.argmax(A3t, axis=1)\n",
        "y_true_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Acurácia\n",
        "acc_test = accuracy_score(y_true_test, y_pred_test)\n",
        "print(f\"Acurácia no conjunto de teste: {acc_test:.4f}\")\n",
        "\n",
        "# Matriz de confusão\n",
        "cm = confusion_matrix(y_true_test, y_pred_test)\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Matriz de Confusão (Teste)')\n",
        "plt.xlabel('Predito')\n",
        "plt.ylabel('Real')\n",
        "plt.show()\n",
        "\n",
        "# Relatório completo (opcional)\n",
        "print(\"\\nRelatório de Classificação:\")\n",
        "print(classification_report(y_true_test, y_pred_test, digits=4, zero_division=1))\n",
        "\n",
        "if epoch == epochs - 1:\n",
        "    print(\"Peso final W1[0,0]:\", W1[0, 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBBUS0RoUlFW"
      },
      "outputs": [],
      "source": [
        "# Hiperparâmetros\n",
        "eta = 0.0001     # teste com outros valores (exercício)\n",
        "epochs = 1500    # teste com outros valores (exercício)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7R0tcA3_UI0E",
        "outputId": "50f66570-18fb-4f68-dd89-920fd4dfb6be"
      },
      "outputs": [],
      "source": [
        "# @title MLP com ReLU nas camadas ocultas e Softmax na saída (SGD - 2 camadas ocultas)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Arquitetura\n",
        "n_input = X_train.shape[1]\n",
        "n_hidden1 = 6\n",
        "n_hidden2 = 3\n",
        "n_output = 2\n",
        "\n",
        "# Funções\n",
        "def relu(z): return np.maximum(0, z)\n",
        "def drelu(z): return (z > 0).astype(float)\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
        "def cross_entropy(y_hat, y_true):\n",
        "    eps = 1e-10\n",
        "    return -np.mean(np.sum(y_true * np.log(y_hat + eps), axis=1))\n",
        "\n",
        "# Inicialização\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(n_input, n_hidden1)\n",
        "b1 = np.zeros((1, n_hidden1))\n",
        "W2 = np.random.randn(n_hidden1, n_hidden2)\n",
        "b2 = np.zeros((1, n_hidden2))\n",
        "W3 = np.random.randn(n_hidden2, n_output)\n",
        "b3 = np.zeros((1, n_output))\n",
        "\n",
        "# Histórico\n",
        "train_loss_history, val_loss_history = [], []\n",
        "train_acc_history, val_acc_history = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # --- Treinamento com SGD\n",
        "    for i in range(X_train.shape[0]):\n",
        "        xi = X_train[i:i+1]\n",
        "        yi = y_train[i:i+1]\n",
        "\n",
        "        # Forward\n",
        "        Z1 = xi @ W1 + b1\n",
        "        A1 = relu(Z1)\n",
        "        Z2 = A1 @ W2 + b2\n",
        "        A2 = relu(Z2)\n",
        "        Z3 = A2 @ W3 + b3\n",
        "        A3 = softmax(Z3)\n",
        "\n",
        "        # Backprop\n",
        "        dZ3 = (A3 - yi)\n",
        "        dW3 = A2.T @ dZ3\n",
        "        db3 = dZ3\n",
        "\n",
        "        dZ2 = (dZ3 @ W3.T) * drelu(Z2)\n",
        "        dW2 = A1.T @ dZ2\n",
        "        db2 = dZ2\n",
        "\n",
        "        dZ1 = (dZ2 @ W2.T) * drelu(Z1)\n",
        "        dW1 = xi.T @ dZ1 # usa apenas uma amostra por vez\n",
        "        db1 = dZ1\n",
        "\n",
        "        # Atualização\n",
        "        W3 -= eta * dW3\n",
        "        b3 -= eta * db3\n",
        "        W2 -= eta * dW2\n",
        "        b2 -= eta * db2\n",
        "        W1 -= eta * dW1\n",
        "        b1 -= eta * db1\n",
        "\n",
        "    # --- Avaliação por época (para gráfico)\n",
        "    # Treino\n",
        "    Z1 = X_train @ W1 + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = A1 @ W2 + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = A2 @ W3 + b3\n",
        "    A3 = softmax(Z3)\n",
        "    train_loss = cross_entropy(A3, y_train)\n",
        "    train_acc = np.mean(np.argmax(A3, axis=1) == np.argmax(y_train, axis=1))\n",
        "\n",
        "    # Validação\n",
        "    Z1v = X_val @ W1 + b1\n",
        "    A1v = relu(Z1v)\n",
        "    Z2v = A1v @ W2 + b2\n",
        "    A2v = relu(Z2v)\n",
        "    Z3v = A2v @ W3 + b3\n",
        "    A3v = softmax(Z3v)\n",
        "    val_loss = cross_entropy(A3v, y_val)\n",
        "    val_acc = np.mean(np.argmax(A3v, axis=1) == np.argmax(y_val, axis=1))\n",
        "\n",
        "    # Registro\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append(val_loss)\n",
        "    train_acc_history.append(train_acc)\n",
        "    val_acc_history.append(val_acc)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# --- Gráficos\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_history, label='Train Loss')\n",
        "plt.plot(val_loss_history, label='Val Loss')\n",
        "plt.title('Perda (SGD)')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Cross-Entropy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc_history, label='Train Acc')\n",
        "plt.plot(val_acc_history, label='Val Acc')\n",
        "plt.title('Acurácia (SGD)')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Forward completo no conjunto de teste\n",
        "Z1t = X_test @ W1 + b1\n",
        "A1t = relu(Z1t)\n",
        "Z2t = A1t @ W2 + b2\n",
        "A2t = relu(Z2t)\n",
        "Z3t = A2t @ W3 + b3\n",
        "A3t = softmax(Z3t)\n",
        "\n",
        "# --- Predição final\n",
        "y_pred_test = np.argmax(A3t, axis=1)\n",
        "y_true_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# --- Acurácia\n",
        "acc_test = accuracy_score(y_true_test, y_pred_test)\n",
        "print(f\"Acurácia no conjunto de teste: {acc_test:.4f}\")\n",
        "\n",
        "# --- Matriz de confusão\n",
        "cm = confusion_matrix(y_true_test, y_pred_test)\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Matriz de Confusão (Teste)')\n",
        "plt.xlabel('Classe Predita')\n",
        "plt.ylabel('Classe Real')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Relatório de classificação\n",
        "print(\"\\nRelatório de Classificação:\")\n",
        "print(classification_report(y_true_test, y_pred_test, digits=4, zero_division=1))\n",
        "\n",
        "if epoch == epochs - 1:\n",
        "    print(\"Peso final W1[0,0]:\", W1[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwGXMW7hXqC7"
      },
      "source": [
        "# **Observação**\n",
        "\n",
        "- O seed é o mesmo $\\implies$ Os pesos iniciais são iguais.\n",
        "- A taxa de aprendizado é muito baixa.\n",
        "- O dataset é pequeno, e o número de épocas alto suaviza o ruído estocástico.\n",
        "\n",
        "Ainda que os resultados sejam \"iguais\" a uma promeira vista, os processos são diferentes e podemos perceber olhando os pesos após o treinamento, por exeplo,\n",
        "\n",
        "- GD: Peso final $W1[0,0]: 0.8271388985463952$.\n",
        "- SGD: Peso final $W1[0,0]: 0.8264546783200991$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA2uGyhGZimf"
      },
      "source": [
        "# **Noções de Redes Convolucionais (CNN) com PyTorch**\n",
        "\n",
        "- As redes convolucionais, Convolutional Neural Networks, (CNNs) são especialmente eficazes para tarefas que envolvem **dados com estrutura espacial**, como imagens.\n",
        "- Sua principal vantagem é a capacidade de **aprender padrões locais** por meio de **filtros convolucionais**, reduzindo o número de parâmetros em comparação com redes totalmente conectadas.\n",
        "\n",
        "---\n",
        "\n",
        "## **Arquitetura de uma CNN**\n",
        "\n",
        "[Site recomendado: Arquitetura de uma CNN](https://alexlenail.me/NN-SVG/LeNet.html)\n",
        "\n",
        "A imagem abaixo ilustra todas as etapas de uma CNN típica, desde a entrada da imagem até a saída de classificação:\n",
        "\n",
        "![CNN](https://cdn.hashnode.com/res/hashnode/image/upload/v1722198375823/1c6c0f55-6748-4c25-ad0d-4cff68b2a5f4.png?auto=compress,format&format=webp)\n",
        "Fonte da figura: sisirdhakal.hashnode.dev\n",
        "\n",
        "**Etapas Representadas:**\n",
        "1. **Entrada**: Imagem de entrada (por exemplo, 28×28 pixels em tons de cinza).\n",
        "2. **Camadas Convolucionais**: Aplicam filtros para extrair características locais.\n",
        "3. **Funções de Ativação (ReLU)**: Introduzem não linearidade.\n",
        "4. **Camadas de Pooling (Max Pooling)**: Reduzem a dimensionalidade espacial.\n",
        "5. **Camada de Flattening**: Transforma os mapas de ativação em um vetor unidimensional.\n",
        "6. **Camadas Totalmente Conectadas**: Realizam a classificação com base nas características extraídas.\n",
        "7. **Camada de Saída (Softmax)**: Fornece as probabilidades associadas a cada classe.\n",
        "\n",
        "## **Componentes fundamentais de uma CNN**\n",
        "\n",
        "1. **Camada Convolucional (`nn.Conv2d`)** Aplica um conjunto de filtros (ou *kernels*) sobre a imagem de entrada. Cada filtro desliza sobre a imagem e gera um mapa de ativação.\n",
        "  - **Quando usar:** Sempre que você quiser extrair padrões locais (bordas, texturas, formas) de imagens ou dados com estrutura espacial.\n",
        "  -   **Função:** Aprende filtros treináveis que identificam características locais (ex: bordas verticais, linhas diagonais, texturas).\n",
        "\n",
        "  ```python\n",
        "  nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
        "  ```\n",
        "  - `in_channels=1` Número de canais de entrada. Uma imagem em escala de cinza tem 1 canal, uma imagem RGB tem 3.\n",
        "  - `out_channels=8` Número de filtros (kernels) que a camada irá aprender. Cada filtro gera um mapa de ativação. Com 8 filtros, a saída terá 8 canais.\n",
        "  - `kernel_size=3` Tamanho dos filtros convolucionais: $3 \\times 3$.Isso significa que cada filtro examina uma vizinhança $3 \\times 3$ da imagem em cada passo.\n",
        "\n",
        "2. **Função de ativação, exemplo `(nn.ReLU)`**\n",
        "  - **Quando usar:** Sempre após camadas convolucionais ou lineares, para adicionar não linearidade ao modelo.\n",
        "  - **Função:** Permite que a rede aprenda funções não lineares, aumentando seu poder de representação.\n",
        "\n",
        "3. **Pooling, exemplo `(nn.MaxPool2d)`** Reduz a dimensionalidade espacial (resolução), mantendo os valores máximos locais.\n",
        "  - **Quando usar:** Logo após uma convolução + ativação, para reduzir a resolução espacial (dimensão da imagem) e manter as informações mais fortes.\n",
        "  - **Função:** Reduz a complexidade e o número de parâmetros, tornando a rede mais eficiente e mais robusta a pequenas variações de posição (invariância local).\n",
        "\n",
        "  ```python\n",
        "  nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "  ```\n",
        "  - `kernel_size=2` Define o tamanho da janela de pooling: $2 \\times 2$. A camada seleciona o maior valor dentro de cada janela $2 \\times 2$ da entrada.\n",
        "  - `stride=2` Define o passo de deslocamento da janela: pula de 2 em 2 pixels. Isso reduz a dimensão da imagem pela metade (subamostragem).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13yqD743tKV7"
      },
      "source": [
        "### **Dataset MNIST**\n",
        "\n",
        "- O Modified National Institute of Standards and Technology (MNIST) é um dos conjuntos de dados mais clássicos e utilizados para a introdução ao aprendizado profundo e CNN.\n",
        "- Ele contém imagens de **dígitos manuscritos** de 0 a 9, escritas por diferentes pessoas.\n",
        "\n",
        "[Site recomendado: Arquitetura de uma CNN](https://alexlenail.me/NN-SVG/LeNet.html)\n",
        "\n",
        "A imagem abaixo ilustra todas as etapas de uma CNN típica, desde a entrada da imagem até a saída de classificação:\n",
        "\n",
        "![MNIST](https://www.mdpi.com/applsci/applsci-09-03169/article_deploy/html/images/applsci-09-03169-g001.png)\n",
        "Fonte da figura: https://www.mdpi.com/2076-3417/9/15/3169\n",
        "\n",
        "---\n",
        "\n",
        "#### **Características do MNIST**\n",
        "\n",
        "- **Número de classes**: 10 (dígitos de `0` a `9`)\n",
        "- **Número de imagens de treino**: 60.000\n",
        "- **Número de imagens de teste**: 10.000\n",
        "- **Formato das imagens**:  \n",
        "  - Tamanho: $28 \\times 28$ pixels  \n",
        "  - Canais: 1 (tons de cinza, escala de 0 a 255)\n",
        "- **Tipo dos dados**: imagens rotuladas\n",
        "\n",
        "#### **Objetivo da Tarefa**\n",
        "\n",
        "Treinar um modelo capaz de **reconhecer automaticamente o dígito manuscrito** em uma imagem, mesmo com diferentes estilos de escrita.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "8iIKdgLfN4vr",
        "outputId": "d5d6872c-382d-4d88-f6b7-86b6b5888751"
      },
      "outputs": [],
      "source": [
        "# @title CNN - Exemplo com MNIST\n",
        "\n",
        "# Importação das bibliotecas principais\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Configuração do dispositivo: GPU (se disponível) ou CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pré-processamento das imagens\n",
        "# - Converte para tensor\n",
        "# - Normaliza com média e desvio padrão da base MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Carregamento do conjunto de treino completo e de teste\n",
        "full_train = datasets.MNIST(root='.', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='.', train=False, transform=transform)\n",
        "\n",
        "# Divisão do conjunto de treino em treino (50.000) e validação (10.000)\n",
        "train_dataset, val_dataset = random_split(full_train, [50000, 10000])\n",
        "\n",
        "# Criação dos DataLoaders (permitem leitura em mini-batches)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Definição da arquitetura da rede convolucional (CNN)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Primeira camada convolucional: 1 canal de entrada, 8 filtros 3x3\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
        "        # Segunda camada convolucional: 8 canais de entrada, 16 filtros 3x3\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
        "        # Camada de pooling para reduzir a dimensionalidade (janela 2x2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Camada totalmente conectada (após achatar para vetor)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)  # 10 classes (dígitos de 0 a 9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Aplicação da sequência: convolução → ReLU → pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # Achata a saída para vetor unidimensional\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        # Camadas densas com ativação ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # Saída final sem ativação (CrossEntropy já aplica Softmax internamente)\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Instancia o modelo, define a função de perda e o otimizador\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()  # Perda para classificação multiclasse\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Parâmetros de treinamento\n",
        "epochs = 5\n",
        "train_loss_hist, val_loss_hist = [], []\n",
        "train_acc_hist, val_acc_hist = [], []\n",
        "\n",
        "# Loop de treinamento\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Modo treino\n",
        "    train_loss, train_correct = 0, 0\n",
        "\n",
        "    # Treinamento por mini-batches\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()           # Zera gradientes anteriores\n",
        "        out = model(x)                  # Faz a previsão\n",
        "        loss = criterion(out, y)        # Calcula o erro\n",
        "        loss.backward()                 # Backpropagation\n",
        "        optimizer.step()                # Atualiza os pesos\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        train_correct += (out.argmax(1) == y).sum().item()\n",
        "\n",
        "    # Validação (sem atualização de pesos)\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            val_loss += loss.item() * x.size(0)\n",
        "            val_correct += (out.argmax(1) == y).sum().item()\n",
        "\n",
        "    # Armazena resultados\n",
        "    train_loss_hist.append(train_loss / len(train_dataset))\n",
        "    val_loss_hist.append(val_loss / len(val_dataset))\n",
        "    train_acc_hist.append(train_correct / len(train_dataset))\n",
        "    val_acc_hist.append(val_correct / len(val_dataset))\n",
        "\n",
        "    # Exibe progresso\n",
        "    print(f\"Época {epoch+1}/{epochs} | Loss Treino: {train_loss_hist[-1]:.4f} | Loss Val: {val_loss_hist[-1]:.4f} | Acc Treino: {train_acc_hist[-1]*100:.2f}% | Acc Val: {val_acc_hist[-1]*100:.2f}%\")\n",
        "\n",
        "# Gráficos de desempenho por época\n",
        "plt.figure(figsize=(8,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss_hist, label=\"Treino\")\n",
        "plt.plot(val_loss_hist, label=\"Validação\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Função de Perda\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_acc_hist, label=\"Treino\")\n",
        "plt.plot(val_acc_hist, label=\"Validação\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Acurácia\")\n",
        "plt.title(\"Acurácia\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "zPwtlRGbxdRf",
        "outputId": "b248427c-3ba6-428c-b75c-444c30efc80f"
      },
      "outputs": [],
      "source": [
        "# @title Avaliação no conjunto de teste\n",
        "\n",
        "# Avaliação no conjunto de teste\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        out = model(x)\n",
        "        preds = out.argmax(1).cpu()\n",
        "        y_true.extend(y.tolist())\n",
        "        y_pred.extend(preds.tolist())\n",
        "\n",
        "# Relatório de classificação com precisão, recall e F1-score\n",
        "print(\"Relatório de Classificação:\")\n",
        "print(classification_report(y_pred, y_true, digits=4))\n",
        "\n",
        "# Matriz de confusão\n",
        "conf = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(conf, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Matriz de Confusão - Conjunto de Teste\")\n",
        "plt.xlabel(\"Classe Real\")\n",
        "plt.ylabel(\"Classe Predita\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMiHC8U6J3xt1tv/Sj5bZiH",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
