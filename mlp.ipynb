{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42428bf",
   "metadata": {},
   "source": [
    "### Preparação do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609d17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cef126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import time\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d47673",
   "metadata": {},
   "source": [
    "### Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccb3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs dos dados\n",
    "url_train = \"https://raw.githubusercontent.com/pcbrom/perceptron-mlp-cnn/refs/heads/main/data/train.csv\"\n",
    "url_test = \"https://raw.githubusercontent.com/pcbrom/perceptron-mlp-cnn/refs/heads/main/data/test.csv\"\n",
    "url_validation = \"https://raw.githubusercontent.com/pcbrom/perceptron-mlp-cnn/refs/heads/main/data/validation.csv\"\n",
    "\n",
    "# Leitura dos dados e seleção das 9 primeiras colunas\n",
    "df_train = pd.read_csv(url_train).iloc[:, :9]\n",
    "df_test = pd.read_csv(url_test).iloc[:, :9]\n",
    "df_validation = pd.read_csv(url_validation).iloc[:, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a3084",
   "metadata": {},
   "source": [
    "### Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5412adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputação dos valores ausentes com a mediana\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Padronização dos dados\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Separação entre features (X) e alvo (y) para cada dataset\n",
    "X_train = df_train.drop(\"Outcome\", axis=1)\n",
    "y_train = df_train[\"Outcome\"]\n",
    "\n",
    "X_test = df_test.drop(\"Outcome\", axis=1)\n",
    "y_test = df_test[\"Outcome\"]\n",
    "\n",
    "X_val = df_validation.drop(\"Outcome\", axis=1)\n",
    "y_val = df_validation[\"Outcome\"]\n",
    "\n",
    "# Imputação de dados ausentes e padronização\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "\n",
    "# Padronização dos dados\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c3b27",
   "metadata": {},
   "source": [
    "### MLP\n",
    "1. Forward pass: Propagação dos dados pela rede (entradas → camadas ocultas → saída).\n",
    "\n",
    "2. Backpropagation: Cálculo do gradiente e atualização dos pesos (algoritmo de retropropagação).\n",
    "\n",
    "3. Treinamento: Usamos o gradiente descendente para otimizar os pesos da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91a99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLayer Perceptron\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01, random_state=42):\n",
    "        # Inicializa as camadas e os pesos\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Define seed para reprodutibilidade\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        # Inicialização Xavier/Glorot para melhor convergência\n",
    "        self.W1 = np.random.randn(self.input_dim, self.hidden_dim) * np.sqrt(2.0 / self.input_dim)\n",
    "        self.b1 = np.zeros((1, self.hidden_dim))\n",
    "        self.W2 = np.random.randn(self.hidden_dim, self.output_dim) * np.sqrt(2.0 / self.hidden_dim)\n",
    "        self.b2 = np.zeros((1, self.output_dim))\n",
    "        \n",
    "        # Histórico de loss para acompanhar convergência\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        # Função de ativação sigmoide com clipping para evitar overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        # Derivada da função sigmoide\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Passagem para frente\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        # Backpropagation\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Reshape y para garantir dimensões corretas\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "            \n",
    "        self.output_error = self.a2 - y\n",
    "        self.dZ2 = self.output_error * self.sigmoid_derivative(self.a2)\n",
    "        self.dW2 = np.dot(self.a1.T, self.dZ2) / m\n",
    "        self.db2 = np.sum(self.dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "        self.dZ1 = np.dot(self.dZ2, self.W2.T) * self.sigmoid_derivative(self.a1)\n",
    "        self.dW1 = np.dot(X.T, self.dZ1) / m\n",
    "        self.db1 = np.sum(self.dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "    def update_weights(self):\n",
    "        # Atualiza os pesos e vieses\n",
    "        self.W1 -= self.learning_rate * self.dW1\n",
    "        self.b1 -= self.learning_rate * self.db1\n",
    "        self.W2 -= self.learning_rate * self.dW2\n",
    "        self.b2 -= self.learning_rate * self.db2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Binary cross-entropy loss (mais apropriado para classificação binária)\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        \n",
    "        # Clipping para evitar log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        # Treinamento da rede neural\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "            self.update_weights()\n",
    "            \n",
    "            # Calcula e armazena a loss\n",
    "            loss = self.compute_loss(y, self.a2)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predições\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def predict_classes(self, X, threshold=0.5):\n",
    "        # Predições de classe\n",
    "        probabilities = self.predict(X)\n",
    "        return (probabilities >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a009d",
   "metadata": {},
   "source": [
    "### Treinamento e avaliação do MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50502ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 1.0681\n",
      "Epoch 100/1000, Loss: 0.9533\n",
      "Epoch 200/1000, Loss: 0.8534\n",
      "Epoch 300/1000, Loss: 0.7739\n",
      "Epoch 400/1000, Loss: 0.7151\n",
      "Epoch 500/1000, Loss: 0.6737\n",
      "Epoch 600/1000, Loss: 0.6453\n",
      "Epoch 700/1000, Loss: 0.6257\n",
      "Epoch 800/1000, Loss: 0.6121\n",
      "Epoch 900/1000, Loss: 0.6024\n",
      "\n",
      "Avaliação do MLP:\n",
      "Acurácia: 0.6779661016949152\n",
      "Precisão: 0.5\n",
      "Revocação (Recall): 0.10526315789473684\n",
      "F1-score: 0.17391304347826086\n"
     ]
    }
   ],
   "source": [
    "# Inicializando e treinando o MLP\n",
    "mlp = MLP(input_dim=X_train_scaled.shape[1], hidden_dim=8, output_dim=1, learning_rate=0.01)\n",
    "\n",
    "# Treinamento (usando 1000 épocas) - convertendo Series para array NumPy\n",
    "mlp.train(X_train_scaled, y_train.values, epochs=1000)\n",
    "\n",
    "# Previsões com o MLP\n",
    "y_pred_mlp_proba = mlp.predict(X_test_scaled)\n",
    "y_pred_mlp = mlp.predict_classes(X_test_scaled)\n",
    "\n",
    "# Flatten para garantir dimensões corretas\n",
    "y_pred_mlp = y_pred_mlp.flatten()\n",
    "\n",
    "# Avaliação do MLP\n",
    "print(\"\\nAvaliação do MLP:\")\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred_mlp))\n",
    "print(\"Precisão:\", precision_score(y_test, y_pred_mlp))\n",
    "print(\"Revocação (Recall):\", recall_score(y_test, y_pred_mlp))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4027cc3",
   "metadata": {},
   "source": [
    "### Comparação RL e MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7f5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeira parte - Regressão Logística\n",
    "start_time = time.time()\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test_scaled)\n",
    "logreg_time = time.time() - start_time\n",
    "\n",
    "# Avaliação da regressão logística\n",
    "logreg_accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "logreg_precision = precision_score(y_test, y_pred_logreg, zero_division=0)\n",
    "logreg_recall = recall_score(y_test, y_pred_logreg, zero_division=0)\n",
    "logreg_f1 = f1_score(y_test, y_pred_logreg, zero_division=0)\n",
    "\n",
    "# Segunda parte - MLP\n",
    "start_time = time.time()\n",
    "mlp_comparison = MLP(input_dim=X_train_scaled.shape[1], hidden_dim=8, output_dim=1, \n",
    "                    learning_rate=0.01, random_state=42)\n",
    "mlp_comparison.train(X_train_scaled, y_train.values, epochs=1000, verbose=False)\n",
    "y_pred_mlp_comparison = mlp_comparison.predict_classes(X_test_scaled).flatten()\n",
    "mlp_time = time.time() - start_time\n",
    "\n",
    "# Avaliação do MLP\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred_mlp_comparison)\n",
    "mlp_precision = precision_score(y_test, y_pred_mlp_comparison, zero_division=0)\n",
    "mlp_recall = recall_score(y_test, y_pred_mlp_comparison, zero_division=0)\n",
    "mlp_f1 = f1_score(y_test, y_pred_mlp_comparison, zero_division=0)\n",
    "\n",
    "# Exibindo os resultados\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'MLP'],\n",
    "    'Accuracy': [logreg_accuracy, mlp_accuracy],\n",
    "    'Precision': [logreg_precision, mlp_precision],\n",
    "    'Recall': [logreg_recall, mlp_recall],\n",
    "    'F1-Score': [logreg_f1, mlp_f1],\n",
    "    'Time (s)': [logreg_time, mlp_time]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ba40c",
   "metadata": {},
   "source": [
    "### Análise da complexidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155c88e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise de Complexidade Computacional:\n",
      "==================================================\n",
      "Logistic Regression - Time: 0.0054s, Memory: 0.0000MB\n",
      "MLP - Time: 0.1128s, Memory: 0.0000MB\n",
      "\n",
      "Resumo da Comparação:\n",
      "Speedup (MLP vs LogReg): 21.03x mais lento\n",
      "Memory overhead (MLP vs LogReg): 0.00x mais memória\n"
     ]
    }
   ],
   "source": [
    "def measure_complexity(model_func, X, y, model_name):\n",
    "    \"\"\"\n",
    "    Função para medir o uso de memória e tempo de treinamento\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Memória inicial (MB)\n",
    "    initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "    \n",
    "    # Executa o treinamento\n",
    "    model = model_func(X, y)\n",
    "    \n",
    "    # Tempo de execução e memória final (MB)\n",
    "    end_time = time.time()\n",
    "    final_memory = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    memory_usage = final_memory - initial_memory\n",
    "    \n",
    "    print(f\"{model_name} - Time: {execution_time:.4f}s, Memory: {memory_usage:.4f}MB\")\n",
    "    \n",
    "    return execution_time, memory_usage, model\n",
    "\n",
    "def train_logistic_regression(X, y):\n",
    "    \"\"\"Função para treinar regressão logística\"\"\"\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def train_mlp(X, y):\n",
    "    \"\"\"Função para treinar MLP\"\"\"\n",
    "    model = MLP(input_dim=X.shape[1], hidden_dim=8, output_dim=1, \n",
    "               learning_rate=0.01, random_state=42)\n",
    "    model.train(X, y, epochs=1000, verbose=False)\n",
    "    return model\n",
    "\n",
    "# Comparando a complexidade computacional dos modelos\n",
    "print(\"Análise de Complexidade Computacional:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "logreg_time, logreg_memory, logreg_model = measure_complexity(\n",
    "    train_logistic_regression, X_train_scaled, y_train, \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "mlp_time, mlp_memory, mlp_model = measure_complexity(\n",
    "    train_mlp, X_train_scaled, y_train.values, \"MLP\"  # Convertendo para array NumPy\n",
    ")\n",
    "\n",
    "# Resumo da comparação\n",
    "print(\"\\nResumo da Comparação:\")\n",
    "print(f\"Speedup (MLP vs LogReg): {mlp_time/logreg_time:.2f}x mais lento\")\n",
    "print(f\"Memory overhead (MLP vs LogReg): {mlp_memory/max(logreg_memory, 0.001):.2f}x mais memória\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
