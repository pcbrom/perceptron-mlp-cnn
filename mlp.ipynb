{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42428bf",
   "metadata": {},
   "source": [
    "### Preparação do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609d17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cef126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import time\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d47673",
   "metadata": {},
   "source": [
    "### Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccb3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs dos dados\n",
    "url_train = \"https://raw.githubusercontent.com/pcbrom/perceptron-mlp-cnn/refs/heads/main/data/train.csv\"\n",
    "url_test = \"https://raw.githubusercontent.com/pcbrom/perceptron-mlp-cnn/refs/heads/main/data/test.csv\"\n",
    "url_validation = \"https://raw.githubusercontent.com/pcbrom/perceptron-mlp-cnn/refs/heads/main/data/validation.csv\"\n",
    "\n",
    "# Leitura dos dados e seleção das 9 primeiras colunas\n",
    "df_train = pd.read_csv(url_train).iloc[:, :9]\n",
    "df_test = pd.read_csv(url_test).iloc[:, :9]\n",
    "df_validation = pd.read_csv(url_validation).iloc[:, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a3084",
   "metadata": {},
   "source": [
    "### Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5412adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputação dos valores ausentes com a mediana\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Padronização dos dados\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Separação entre features (X) e alvo (y) para cada dataset\n",
    "X_train = df_train.drop(\"Outcome\", axis=1)\n",
    "y_train = df_train[\"Outcome\"]\n",
    "\n",
    "X_test = df_test.drop(\"Outcome\", axis=1)\n",
    "y_test = df_test[\"Outcome\"]\n",
    "\n",
    "X_val = df_validation.drop(\"Outcome\", axis=1)\n",
    "y_val = df_validation[\"Outcome\"]\n",
    "\n",
    "# Imputação de dados ausentes e padronização\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "\n",
    "# Padronização dos dados\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c3b27",
   "metadata": {},
   "source": [
    "### MLP\n",
    "1. Forward pass: Propagação dos dados pela rede (entradas → camadas ocultas → saída).\n",
    "\n",
    "2. Backpropagation: Cálculo do gradiente e atualização dos pesos (algoritmo de retropropagação).\n",
    "\n",
    "3. Treinamento: Usamos o gradiente descendente para otimizar os pesos da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91a99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLayer Perceptron\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01, random_state=42):\n",
    "        # Inicializa as camadas e os pesos\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Define seed para reprodutibilidade\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        # Inicialização Xavier/Glorot para melhor convergência\n",
    "        self.W1 = np.random.randn(self.input_dim, self.hidden_dim) * np.sqrt(2.0 / self.input_dim)\n",
    "        self.b1 = np.zeros((1, self.hidden_dim))\n",
    "        self.W2 = np.random.randn(self.hidden_dim, self.output_dim) * np.sqrt(2.0 / self.hidden_dim)\n",
    "        self.b2 = np.zeros((1, self.output_dim))\n",
    "        \n",
    "        # Histórico de loss para acompanhar convergência\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        # Função de ativação sigmoide com clipping para evitar overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        # Derivada da função sigmoide\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Passagem para frente\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        # Backpropagation\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Reshape y para garantir dimensões corretas\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "            \n",
    "        self.output_error = self.a2 - y\n",
    "        self.dZ2 = self.output_error * self.sigmoid_derivative(self.a2)\n",
    "        self.dW2 = np.dot(self.a1.T, self.dZ2) / m\n",
    "        self.db2 = np.sum(self.dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "        self.dZ1 = np.dot(self.dZ2, self.W2.T) * self.sigmoid_derivative(self.a1)\n",
    "        self.dW1 = np.dot(X.T, self.dZ1) / m\n",
    "        self.db1 = np.sum(self.dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "    def update_weights(self):\n",
    "        # Atualiza os pesos e vieses\n",
    "        self.W1 -= self.learning_rate * self.dW1\n",
    "        self.b1 -= self.learning_rate * self.db1\n",
    "        self.W2 -= self.learning_rate * self.dW2\n",
    "        self.b2 -= self.learning_rate * self.db2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Binary cross-entropy loss (mais apropriado para classificação binária)\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        \n",
    "        # Clipping para evitar log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        # Treinamento da rede neural\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "            self.update_weights()\n",
    "            \n",
    "            # Calcula e armazena a loss\n",
    "            loss = self.compute_loss(y, self.a2)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predições\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def predict_classes(self, X, threshold=0.5):\n",
    "        # Predições de classe\n",
    "        probabilities = self.predict(X)\n",
    "        return (probabilities >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a009d",
   "metadata": {},
   "source": [
    "### Treinamento e avaliação do MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50502ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000, Loss: 1.0681\n",
      "Epoch 100/100000, Loss: 0.9533\n",
      "Epoch 200/100000, Loss: 0.8534\n",
      "Epoch 300/100000, Loss: 0.7739\n",
      "Epoch 400/100000, Loss: 0.7151\n",
      "Epoch 500/100000, Loss: 0.6737\n",
      "Epoch 600/100000, Loss: 0.6453\n",
      "Epoch 700/100000, Loss: 0.6257\n",
      "Epoch 800/100000, Loss: 0.6121\n",
      "Epoch 900/100000, Loss: 0.6024\n",
      "Epoch 1000/100000, Loss: 0.5954\n",
      "Epoch 1100/100000, Loss: 0.5901\n",
      "Epoch 1200/100000, Loss: 0.5860\n",
      "Epoch 1300/100000, Loss: 0.5827\n",
      "Epoch 1400/100000, Loss: 0.5800\n",
      "Epoch 1500/100000, Loss: 0.5778\n",
      "Epoch 1600/100000, Loss: 0.5758\n",
      "Epoch 1700/100000, Loss: 0.5740\n",
      "Epoch 1800/100000, Loss: 0.5724\n",
      "Epoch 1900/100000, Loss: 0.5710\n",
      "Epoch 2000/100000, Loss: 0.5696\n",
      "Epoch 2100/100000, Loss: 0.5683\n",
      "Epoch 2200/100000, Loss: 0.5670\n",
      "Epoch 2300/100000, Loss: 0.5658\n",
      "Epoch 2400/100000, Loss: 0.5646\n",
      "Epoch 2500/100000, Loss: 0.5635\n",
      "Epoch 2600/100000, Loss: 0.5623\n",
      "Epoch 2700/100000, Loss: 0.5612\n",
      "Epoch 2800/100000, Loss: 0.5601\n",
      "Epoch 2900/100000, Loss: 0.5591\n",
      "Epoch 3000/100000, Loss: 0.5580\n",
      "Epoch 3100/100000, Loss: 0.5570\n",
      "Epoch 3200/100000, Loss: 0.5559\n",
      "Epoch 3300/100000, Loss: 0.5549\n",
      "Epoch 3400/100000, Loss: 0.5539\n",
      "Epoch 3500/100000, Loss: 0.5529\n",
      "Epoch 3600/100000, Loss: 0.5519\n",
      "Epoch 3700/100000, Loss: 0.5509\n",
      "Epoch 3800/100000, Loss: 0.5500\n",
      "Epoch 3900/100000, Loss: 0.5490\n",
      "Epoch 4000/100000, Loss: 0.5480\n",
      "Epoch 4100/100000, Loss: 0.5471\n",
      "Epoch 4200/100000, Loss: 0.5462\n",
      "Epoch 4300/100000, Loss: 0.5452\n",
      "Epoch 4400/100000, Loss: 0.5443\n",
      "Epoch 4500/100000, Loss: 0.5434\n",
      "Epoch 4600/100000, Loss: 0.5425\n",
      "Epoch 4700/100000, Loss: 0.5416\n",
      "Epoch 4800/100000, Loss: 0.5407\n",
      "Epoch 4900/100000, Loss: 0.5398\n",
      "Epoch 5000/100000, Loss: 0.5389\n",
      "Epoch 5100/100000, Loss: 0.5380\n",
      "Epoch 5200/100000, Loss: 0.5372\n",
      "Epoch 5300/100000, Loss: 0.5363\n",
      "Epoch 5400/100000, Loss: 0.5354\n",
      "Epoch 5500/100000, Loss: 0.5346\n",
      "Epoch 5600/100000, Loss: 0.5338\n",
      "Epoch 5700/100000, Loss: 0.5329\n",
      "Epoch 5800/100000, Loss: 0.5321\n",
      "Epoch 5900/100000, Loss: 0.5313\n",
      "Epoch 6000/100000, Loss: 0.5305\n",
      "Epoch 6100/100000, Loss: 0.5297\n",
      "Epoch 6200/100000, Loss: 0.5289\n",
      "Epoch 6300/100000, Loss: 0.5281\n",
      "Epoch 6400/100000, Loss: 0.5274\n",
      "Epoch 6500/100000, Loss: 0.5266\n",
      "Epoch 6600/100000, Loss: 0.5258\n",
      "Epoch 6700/100000, Loss: 0.5251\n",
      "Epoch 6800/100000, Loss: 0.5243\n",
      "Epoch 6900/100000, Loss: 0.5236\n",
      "Epoch 7000/100000, Loss: 0.5229\n",
      "Epoch 7100/100000, Loss: 0.5221\n",
      "Epoch 7200/100000, Loss: 0.5214\n",
      "Epoch 7300/100000, Loss: 0.5207\n",
      "Epoch 7400/100000, Loss: 0.5200\n",
      "Epoch 7500/100000, Loss: 0.5193\n",
      "Epoch 7600/100000, Loss: 0.5186\n",
      "Epoch 7700/100000, Loss: 0.5180\n",
      "Epoch 7800/100000, Loss: 0.5173\n",
      "Epoch 7900/100000, Loss: 0.5166\n",
      "Epoch 8000/100000, Loss: 0.5160\n",
      "Epoch 8100/100000, Loss: 0.5153\n",
      "Epoch 8200/100000, Loss: 0.5147\n",
      "Epoch 8300/100000, Loss: 0.5141\n",
      "Epoch 8400/100000, Loss: 0.5134\n",
      "Epoch 8500/100000, Loss: 0.5128\n",
      "Epoch 8600/100000, Loss: 0.5122\n",
      "Epoch 8700/100000, Loss: 0.5116\n",
      "Epoch 8800/100000, Loss: 0.5110\n",
      "Epoch 8900/100000, Loss: 0.5104\n",
      "Epoch 9000/100000, Loss: 0.5098\n",
      "Epoch 9100/100000, Loss: 0.5092\n",
      "Epoch 9200/100000, Loss: 0.5087\n",
      "Epoch 9300/100000, Loss: 0.5081\n",
      "Epoch 9400/100000, Loss: 0.5076\n",
      "Epoch 9500/100000, Loss: 0.5070\n",
      "Epoch 9600/100000, Loss: 0.5065\n",
      "Epoch 9700/100000, Loss: 0.5059\n",
      "Epoch 9800/100000, Loss: 0.5054\n",
      "Epoch 9900/100000, Loss: 0.5049\n",
      "Epoch 10000/100000, Loss: 0.5043\n",
      "Epoch 10100/100000, Loss: 0.5038\n",
      "Epoch 10200/100000, Loss: 0.5033\n",
      "Epoch 10300/100000, Loss: 0.5028\n",
      "Epoch 10400/100000, Loss: 0.5023\n",
      "Epoch 10500/100000, Loss: 0.5018\n",
      "Epoch 10600/100000, Loss: 0.5014\n",
      "Epoch 10700/100000, Loss: 0.5009\n",
      "Epoch 10800/100000, Loss: 0.5004\n",
      "Epoch 10900/100000, Loss: 0.4999\n",
      "Epoch 11000/100000, Loss: 0.4995\n",
      "Epoch 11100/100000, Loss: 0.4990\n",
      "Epoch 11200/100000, Loss: 0.4986\n",
      "Epoch 11300/100000, Loss: 0.4981\n",
      "Epoch 11400/100000, Loss: 0.4977\n",
      "Epoch 11500/100000, Loss: 0.4973\n",
      "Epoch 11600/100000, Loss: 0.4968\n",
      "Epoch 11700/100000, Loss: 0.4964\n",
      "Epoch 11800/100000, Loss: 0.4960\n",
      "Epoch 11900/100000, Loss: 0.4956\n",
      "Epoch 12000/100000, Loss: 0.4952\n",
      "Epoch 12100/100000, Loss: 0.4948\n",
      "Epoch 12200/100000, Loss: 0.4944\n",
      "Epoch 12300/100000, Loss: 0.4940\n",
      "Epoch 12400/100000, Loss: 0.4936\n",
      "Epoch 12500/100000, Loss: 0.4932\n",
      "Epoch 12600/100000, Loss: 0.4928\n",
      "Epoch 12700/100000, Loss: 0.4924\n",
      "Epoch 12800/100000, Loss: 0.4921\n",
      "Epoch 12900/100000, Loss: 0.4917\n",
      "Epoch 13000/100000, Loss: 0.4913\n",
      "Epoch 13100/100000, Loss: 0.4910\n",
      "Epoch 13200/100000, Loss: 0.4906\n",
      "Epoch 13300/100000, Loss: 0.4903\n",
      "Epoch 13400/100000, Loss: 0.4899\n",
      "Epoch 13500/100000, Loss: 0.4896\n",
      "Epoch 13600/100000, Loss: 0.4893\n",
      "Epoch 13700/100000, Loss: 0.4889\n",
      "Epoch 13800/100000, Loss: 0.4886\n",
      "Epoch 13900/100000, Loss: 0.4883\n",
      "Epoch 14000/100000, Loss: 0.4879\n",
      "Epoch 14100/100000, Loss: 0.4876\n",
      "Epoch 14200/100000, Loss: 0.4873\n",
      "Epoch 14300/100000, Loss: 0.4870\n",
      "Epoch 14400/100000, Loss: 0.4867\n",
      "Epoch 14500/100000, Loss: 0.4864\n",
      "Epoch 14600/100000, Loss: 0.4861\n",
      "Epoch 14700/100000, Loss: 0.4858\n",
      "Epoch 14800/100000, Loss: 0.4855\n",
      "Epoch 14900/100000, Loss: 0.4852\n",
      "Epoch 15000/100000, Loss: 0.4849\n",
      "Epoch 15100/100000, Loss: 0.4846\n",
      "Epoch 15200/100000, Loss: 0.4843\n",
      "Epoch 15300/100000, Loss: 0.4841\n",
      "Epoch 15400/100000, Loss: 0.4838\n",
      "Epoch 15500/100000, Loss: 0.4835\n",
      "Epoch 15600/100000, Loss: 0.4832\n",
      "Epoch 15700/100000, Loss: 0.4830\n",
      "Epoch 15800/100000, Loss: 0.4827\n",
      "Epoch 15900/100000, Loss: 0.4824\n",
      "Epoch 16000/100000, Loss: 0.4822\n",
      "Epoch 16100/100000, Loss: 0.4819\n",
      "Epoch 16200/100000, Loss: 0.4817\n",
      "Epoch 16300/100000, Loss: 0.4814\n",
      "Epoch 16400/100000, Loss: 0.4812\n",
      "Epoch 16500/100000, Loss: 0.4809\n",
      "Epoch 16600/100000, Loss: 0.4807\n",
      "Epoch 16700/100000, Loss: 0.4804\n",
      "Epoch 16800/100000, Loss: 0.4802\n",
      "Epoch 16900/100000, Loss: 0.4800\n",
      "Epoch 17000/100000, Loss: 0.4797\n",
      "Epoch 17100/100000, Loss: 0.4795\n",
      "Epoch 17200/100000, Loss: 0.4793\n",
      "Epoch 17300/100000, Loss: 0.4790\n",
      "Epoch 17400/100000, Loss: 0.4788\n",
      "Epoch 17500/100000, Loss: 0.4786\n",
      "Epoch 17600/100000, Loss: 0.4784\n",
      "Epoch 17700/100000, Loss: 0.4781\n",
      "Epoch 17800/100000, Loss: 0.4779\n",
      "Epoch 17900/100000, Loss: 0.4777\n",
      "Epoch 18000/100000, Loss: 0.4775\n",
      "Epoch 18100/100000, Loss: 0.4773\n",
      "Epoch 18200/100000, Loss: 0.4771\n",
      "Epoch 18300/100000, Loss: 0.4769\n",
      "Epoch 18400/100000, Loss: 0.4767\n",
      "Epoch 18500/100000, Loss: 0.4765\n",
      "Epoch 18600/100000, Loss: 0.4763\n",
      "Epoch 18700/100000, Loss: 0.4761\n",
      "Epoch 18800/100000, Loss: 0.4759\n",
      "Epoch 18900/100000, Loss: 0.4757\n",
      "Epoch 19000/100000, Loss: 0.4755\n",
      "Epoch 19100/100000, Loss: 0.4753\n",
      "Epoch 19200/100000, Loss: 0.4751\n",
      "Epoch 19300/100000, Loss: 0.4749\n",
      "Epoch 19400/100000, Loss: 0.4747\n",
      "Epoch 19500/100000, Loss: 0.4745\n",
      "Epoch 19600/100000, Loss: 0.4743\n",
      "Epoch 19700/100000, Loss: 0.4741\n",
      "Epoch 19800/100000, Loss: 0.4740\n",
      "Epoch 19900/100000, Loss: 0.4738\n",
      "Epoch 20000/100000, Loss: 0.4736\n",
      "Epoch 20100/100000, Loss: 0.4734\n",
      "Epoch 20200/100000, Loss: 0.4733\n",
      "Epoch 20300/100000, Loss: 0.4731\n",
      "Epoch 20400/100000, Loss: 0.4729\n",
      "Epoch 20500/100000, Loss: 0.4727\n",
      "Epoch 20600/100000, Loss: 0.4726\n",
      "Epoch 20700/100000, Loss: 0.4724\n",
      "Epoch 20800/100000, Loss: 0.4722\n",
      "Epoch 20900/100000, Loss: 0.4721\n",
      "Epoch 21000/100000, Loss: 0.4719\n",
      "Epoch 21100/100000, Loss: 0.4717\n",
      "Epoch 21200/100000, Loss: 0.4716\n",
      "Epoch 21300/100000, Loss: 0.4714\n",
      "Epoch 21400/100000, Loss: 0.4712\n",
      "Epoch 21500/100000, Loss: 0.4711\n",
      "Epoch 21600/100000, Loss: 0.4709\n",
      "Epoch 21700/100000, Loss: 0.4708\n",
      "Epoch 21800/100000, Loss: 0.4706\n",
      "Epoch 21900/100000, Loss: 0.4705\n",
      "Epoch 22000/100000, Loss: 0.4703\n",
      "Epoch 22100/100000, Loss: 0.4702\n",
      "Epoch 22200/100000, Loss: 0.4700\n",
      "Epoch 22300/100000, Loss: 0.4699\n",
      "Epoch 22400/100000, Loss: 0.4697\n",
      "Epoch 22500/100000, Loss: 0.4696\n",
      "Epoch 22600/100000, Loss: 0.4694\n",
      "Epoch 22700/100000, Loss: 0.4693\n",
      "Epoch 22800/100000, Loss: 0.4691\n",
      "Epoch 22900/100000, Loss: 0.4690\n",
      "Epoch 23000/100000, Loss: 0.4688\n",
      "Epoch 23100/100000, Loss: 0.4687\n",
      "Epoch 23200/100000, Loss: 0.4685\n",
      "Epoch 23300/100000, Loss: 0.4684\n",
      "Epoch 23400/100000, Loss: 0.4683\n",
      "Epoch 23500/100000, Loss: 0.4681\n",
      "Epoch 23600/100000, Loss: 0.4680\n",
      "Epoch 23700/100000, Loss: 0.4679\n",
      "Epoch 23800/100000, Loss: 0.4677\n",
      "Epoch 23900/100000, Loss: 0.4676\n",
      "Epoch 24000/100000, Loss: 0.4674\n",
      "Epoch 24100/100000, Loss: 0.4673\n",
      "Epoch 24200/100000, Loss: 0.4672\n",
      "Epoch 24300/100000, Loss: 0.4671\n",
      "Epoch 24400/100000, Loss: 0.4669\n",
      "Epoch 24500/100000, Loss: 0.4668\n",
      "Epoch 24600/100000, Loss: 0.4667\n",
      "Epoch 24700/100000, Loss: 0.4665\n",
      "Epoch 24800/100000, Loss: 0.4664\n",
      "Epoch 24900/100000, Loss: 0.4663\n",
      "Epoch 25000/100000, Loss: 0.4662\n",
      "Epoch 25100/100000, Loss: 0.4660\n",
      "Epoch 25200/100000, Loss: 0.4659\n",
      "Epoch 25300/100000, Loss: 0.4658\n",
      "Epoch 25400/100000, Loss: 0.4657\n",
      "Epoch 25500/100000, Loss: 0.4655\n",
      "Epoch 25600/100000, Loss: 0.4654\n",
      "Epoch 25700/100000, Loss: 0.4653\n",
      "Epoch 25800/100000, Loss: 0.4652\n",
      "Epoch 25900/100000, Loss: 0.4651\n",
      "Epoch 26000/100000, Loss: 0.4649\n",
      "Epoch 26100/100000, Loss: 0.4648\n",
      "Epoch 26200/100000, Loss: 0.4647\n",
      "Epoch 26300/100000, Loss: 0.4646\n",
      "Epoch 26400/100000, Loss: 0.4645\n",
      "Epoch 26500/100000, Loss: 0.4644\n",
      "Epoch 26600/100000, Loss: 0.4642\n",
      "Epoch 26700/100000, Loss: 0.4641\n",
      "Epoch 26800/100000, Loss: 0.4640\n",
      "Epoch 26900/100000, Loss: 0.4639\n",
      "Epoch 27000/100000, Loss: 0.4638\n",
      "Epoch 27100/100000, Loss: 0.4637\n",
      "Epoch 27200/100000, Loss: 0.4636\n",
      "Epoch 27300/100000, Loss: 0.4635\n",
      "Epoch 27400/100000, Loss: 0.4633\n",
      "Epoch 27500/100000, Loss: 0.4632\n",
      "Epoch 27600/100000, Loss: 0.4631\n",
      "Epoch 27700/100000, Loss: 0.4630\n",
      "Epoch 27800/100000, Loss: 0.4629\n",
      "Epoch 27900/100000, Loss: 0.4628\n",
      "Epoch 28000/100000, Loss: 0.4627\n",
      "Epoch 28100/100000, Loss: 0.4626\n",
      "Epoch 28200/100000, Loss: 0.4625\n",
      "Epoch 28300/100000, Loss: 0.4624\n",
      "Epoch 28400/100000, Loss: 0.4623\n",
      "Epoch 28500/100000, Loss: 0.4622\n",
      "Epoch 28600/100000, Loss: 0.4621\n",
      "Epoch 28700/100000, Loss: 0.4620\n",
      "Epoch 28800/100000, Loss: 0.4619\n",
      "Epoch 28900/100000, Loss: 0.4618\n",
      "Epoch 29000/100000, Loss: 0.4617\n",
      "Epoch 29100/100000, Loss: 0.4616\n",
      "Epoch 29200/100000, Loss: 0.4615\n",
      "Epoch 29300/100000, Loss: 0.4614\n",
      "Epoch 29400/100000, Loss: 0.4613\n",
      "Epoch 29500/100000, Loss: 0.4612\n",
      "Epoch 29600/100000, Loss: 0.4611\n",
      "Epoch 29700/100000, Loss: 0.4610\n",
      "Epoch 29800/100000, Loss: 0.4609\n",
      "Epoch 29900/100000, Loss: 0.4608\n",
      "Epoch 30000/100000, Loss: 0.4607\n",
      "Epoch 30100/100000, Loss: 0.4606\n",
      "Epoch 30200/100000, Loss: 0.4605\n",
      "Epoch 30300/100000, Loss: 0.4604\n",
      "Epoch 30400/100000, Loss: 0.4603\n",
      "Epoch 30500/100000, Loss: 0.4602\n",
      "Epoch 30600/100000, Loss: 0.4601\n",
      "Epoch 30700/100000, Loss: 0.4600\n",
      "Epoch 30800/100000, Loss: 0.4599\n",
      "Epoch 30900/100000, Loss: 0.4598\n",
      "Epoch 31000/100000, Loss: 0.4597\n",
      "Epoch 31100/100000, Loss: 0.4596\n",
      "Epoch 31200/100000, Loss: 0.4596\n",
      "Epoch 31300/100000, Loss: 0.4595\n",
      "Epoch 31400/100000, Loss: 0.4594\n",
      "Epoch 31500/100000, Loss: 0.4593\n",
      "Epoch 31600/100000, Loss: 0.4592\n",
      "Epoch 31700/100000, Loss: 0.4591\n",
      "Epoch 31800/100000, Loss: 0.4590\n",
      "Epoch 31900/100000, Loss: 0.4589\n",
      "Epoch 32000/100000, Loss: 0.4588\n",
      "Epoch 32100/100000, Loss: 0.4587\n",
      "Epoch 32200/100000, Loss: 0.4587\n",
      "Epoch 32300/100000, Loss: 0.4586\n",
      "Epoch 32400/100000, Loss: 0.4585\n",
      "Epoch 32500/100000, Loss: 0.4584\n",
      "Epoch 32600/100000, Loss: 0.4583\n",
      "Epoch 32700/100000, Loss: 0.4582\n",
      "Epoch 32800/100000, Loss: 0.4581\n",
      "Epoch 32900/100000, Loss: 0.4581\n",
      "Epoch 33000/100000, Loss: 0.4580\n",
      "Epoch 33100/100000, Loss: 0.4579\n",
      "Epoch 33200/100000, Loss: 0.4578\n",
      "Epoch 33300/100000, Loss: 0.4577\n",
      "Epoch 33400/100000, Loss: 0.4576\n",
      "Epoch 33500/100000, Loss: 0.4576\n",
      "Epoch 33600/100000, Loss: 0.4575\n",
      "Epoch 33700/100000, Loss: 0.4574\n",
      "Epoch 33800/100000, Loss: 0.4573\n",
      "Epoch 33900/100000, Loss: 0.4572\n",
      "Epoch 34000/100000, Loss: 0.4571\n",
      "Epoch 34100/100000, Loss: 0.4571\n",
      "Epoch 34200/100000, Loss: 0.4570\n",
      "Epoch 34300/100000, Loss: 0.4569\n",
      "Epoch 34400/100000, Loss: 0.4568\n",
      "Epoch 34500/100000, Loss: 0.4567\n",
      "Epoch 34600/100000, Loss: 0.4567\n",
      "Epoch 34700/100000, Loss: 0.4566\n",
      "Epoch 34800/100000, Loss: 0.4565\n",
      "Epoch 34900/100000, Loss: 0.4564\n",
      "Epoch 35000/100000, Loss: 0.4563\n",
      "Epoch 35100/100000, Loss: 0.4563\n",
      "Epoch 35200/100000, Loss: 0.4562\n",
      "Epoch 35300/100000, Loss: 0.4561\n",
      "Epoch 35400/100000, Loss: 0.4560\n",
      "Epoch 35500/100000, Loss: 0.4559\n",
      "Epoch 35600/100000, Loss: 0.4559\n",
      "Epoch 35700/100000, Loss: 0.4558\n",
      "Epoch 35800/100000, Loss: 0.4557\n",
      "Epoch 35900/100000, Loss: 0.4556\n",
      "Epoch 36000/100000, Loss: 0.4556\n",
      "Epoch 36100/100000, Loss: 0.4555\n",
      "Epoch 36200/100000, Loss: 0.4554\n",
      "Epoch 36300/100000, Loss: 0.4553\n",
      "Epoch 36400/100000, Loss: 0.4553\n",
      "Epoch 36500/100000, Loss: 0.4552\n",
      "Epoch 36600/100000, Loss: 0.4551\n",
      "Epoch 36700/100000, Loss: 0.4550\n",
      "Epoch 36800/100000, Loss: 0.4550\n",
      "Epoch 36900/100000, Loss: 0.4549\n",
      "Epoch 37000/100000, Loss: 0.4548\n",
      "Epoch 37100/100000, Loss: 0.4547\n",
      "Epoch 37200/100000, Loss: 0.4547\n",
      "Epoch 37300/100000, Loss: 0.4546\n",
      "Epoch 37400/100000, Loss: 0.4545\n",
      "Epoch 37500/100000, Loss: 0.4545\n",
      "Epoch 37600/100000, Loss: 0.4544\n",
      "Epoch 37700/100000, Loss: 0.4543\n",
      "Epoch 37800/100000, Loss: 0.4542\n",
      "Epoch 37900/100000, Loss: 0.4542\n",
      "Epoch 38000/100000, Loss: 0.4541\n",
      "Epoch 38100/100000, Loss: 0.4540\n",
      "Epoch 38200/100000, Loss: 0.4540\n",
      "Epoch 38300/100000, Loss: 0.4539\n",
      "Epoch 38400/100000, Loss: 0.4538\n",
      "Epoch 38500/100000, Loss: 0.4537\n",
      "Epoch 38600/100000, Loss: 0.4537\n",
      "Epoch 38700/100000, Loss: 0.4536\n",
      "Epoch 38800/100000, Loss: 0.4535\n",
      "Epoch 38900/100000, Loss: 0.4535\n",
      "Epoch 39000/100000, Loss: 0.4534\n",
      "Epoch 39100/100000, Loss: 0.4533\n",
      "Epoch 39200/100000, Loss: 0.4533\n",
      "Epoch 39300/100000, Loss: 0.4532\n",
      "Epoch 39400/100000, Loss: 0.4531\n",
      "Epoch 39500/100000, Loss: 0.4531\n",
      "Epoch 39600/100000, Loss: 0.4530\n",
      "Epoch 39700/100000, Loss: 0.4529\n",
      "Epoch 39800/100000, Loss: 0.4528\n",
      "Epoch 39900/100000, Loss: 0.4528\n",
      "Epoch 40000/100000, Loss: 0.4527\n",
      "Epoch 40100/100000, Loss: 0.4526\n",
      "Epoch 40200/100000, Loss: 0.4526\n",
      "Epoch 40300/100000, Loss: 0.4525\n",
      "Epoch 40400/100000, Loss: 0.4524\n",
      "Epoch 40500/100000, Loss: 0.4524\n",
      "Epoch 40600/100000, Loss: 0.4523\n",
      "Epoch 40700/100000, Loss: 0.4522\n",
      "Epoch 40800/100000, Loss: 0.4522\n",
      "Epoch 40900/100000, Loss: 0.4521\n",
      "Epoch 41000/100000, Loss: 0.4521\n",
      "Epoch 41100/100000, Loss: 0.4520\n",
      "Epoch 41200/100000, Loss: 0.4519\n",
      "Epoch 41300/100000, Loss: 0.4519\n",
      "Epoch 41400/100000, Loss: 0.4518\n",
      "Epoch 41500/100000, Loss: 0.4517\n",
      "Epoch 41600/100000, Loss: 0.4517\n",
      "Epoch 41700/100000, Loss: 0.4516\n",
      "Epoch 41800/100000, Loss: 0.4515\n",
      "Epoch 41900/100000, Loss: 0.4515\n",
      "Epoch 42000/100000, Loss: 0.4514\n",
      "Epoch 42100/100000, Loss: 0.4513\n",
      "Epoch 42200/100000, Loss: 0.4513\n",
      "Epoch 42300/100000, Loss: 0.4512\n",
      "Epoch 42400/100000, Loss: 0.4511\n",
      "Epoch 42500/100000, Loss: 0.4511\n",
      "Epoch 42600/100000, Loss: 0.4510\n",
      "Epoch 42700/100000, Loss: 0.4510\n",
      "Epoch 42800/100000, Loss: 0.4509\n",
      "Epoch 42900/100000, Loss: 0.4508\n",
      "Epoch 43000/100000, Loss: 0.4508\n",
      "Epoch 43100/100000, Loss: 0.4507\n",
      "Epoch 43200/100000, Loss: 0.4506\n",
      "Epoch 43300/100000, Loss: 0.4506\n",
      "Epoch 43400/100000, Loss: 0.4505\n",
      "Epoch 43500/100000, Loss: 0.4505\n",
      "Epoch 43600/100000, Loss: 0.4504\n",
      "Epoch 43700/100000, Loss: 0.4503\n",
      "Epoch 43800/100000, Loss: 0.4503\n",
      "Epoch 43900/100000, Loss: 0.4502\n",
      "Epoch 44000/100000, Loss: 0.4502\n",
      "Epoch 44100/100000, Loss: 0.4501\n",
      "Epoch 44200/100000, Loss: 0.4500\n",
      "Epoch 44300/100000, Loss: 0.4500\n",
      "Epoch 44400/100000, Loss: 0.4499\n",
      "Epoch 44500/100000, Loss: 0.4499\n",
      "Epoch 44600/100000, Loss: 0.4498\n",
      "Epoch 44700/100000, Loss: 0.4497\n",
      "Epoch 44800/100000, Loss: 0.4497\n",
      "Epoch 44900/100000, Loss: 0.4496\n",
      "Epoch 45000/100000, Loss: 0.4496\n",
      "Epoch 45100/100000, Loss: 0.4495\n",
      "Epoch 45200/100000, Loss: 0.4494\n",
      "Epoch 45300/100000, Loss: 0.4494\n",
      "Epoch 45400/100000, Loss: 0.4493\n",
      "Epoch 45500/100000, Loss: 0.4493\n",
      "Epoch 45600/100000, Loss: 0.4492\n",
      "Epoch 45700/100000, Loss: 0.4491\n",
      "Epoch 45800/100000, Loss: 0.4491\n",
      "Epoch 45900/100000, Loss: 0.4490\n",
      "Epoch 46000/100000, Loss: 0.4490\n",
      "Epoch 46100/100000, Loss: 0.4489\n",
      "Epoch 46200/100000, Loss: 0.4488\n",
      "Epoch 46300/100000, Loss: 0.4488\n",
      "Epoch 46400/100000, Loss: 0.4487\n",
      "Epoch 46500/100000, Loss: 0.4487\n",
      "Epoch 46600/100000, Loss: 0.4486\n",
      "Epoch 46700/100000, Loss: 0.4486\n",
      "Epoch 46800/100000, Loss: 0.4485\n",
      "Epoch 46900/100000, Loss: 0.4484\n",
      "Epoch 47000/100000, Loss: 0.4484\n",
      "Epoch 47100/100000, Loss: 0.4483\n",
      "Epoch 47200/100000, Loss: 0.4483\n",
      "Epoch 47300/100000, Loss: 0.4482\n",
      "Epoch 47400/100000, Loss: 0.4482\n",
      "Epoch 47500/100000, Loss: 0.4481\n",
      "Epoch 47600/100000, Loss: 0.4480\n",
      "Epoch 47700/100000, Loss: 0.4480\n",
      "Epoch 47800/100000, Loss: 0.4479\n",
      "Epoch 47900/100000, Loss: 0.4479\n",
      "Epoch 48000/100000, Loss: 0.4478\n",
      "Epoch 48100/100000, Loss: 0.4478\n",
      "Epoch 48200/100000, Loss: 0.4477\n",
      "Epoch 48300/100000, Loss: 0.4477\n",
      "Epoch 48400/100000, Loss: 0.4476\n",
      "Epoch 48500/100000, Loss: 0.4475\n",
      "Epoch 48600/100000, Loss: 0.4475\n",
      "Epoch 48700/100000, Loss: 0.4474\n",
      "Epoch 48800/100000, Loss: 0.4474\n",
      "Epoch 48900/100000, Loss: 0.4473\n",
      "Epoch 49000/100000, Loss: 0.4473\n",
      "Epoch 49100/100000, Loss: 0.4472\n",
      "Epoch 49200/100000, Loss: 0.4472\n",
      "Epoch 49300/100000, Loss: 0.4471\n",
      "Epoch 49400/100000, Loss: 0.4470\n",
      "Epoch 49500/100000, Loss: 0.4470\n",
      "Epoch 49600/100000, Loss: 0.4469\n",
      "Epoch 49700/100000, Loss: 0.4469\n",
      "Epoch 49800/100000, Loss: 0.4468\n",
      "Epoch 49900/100000, Loss: 0.4468\n",
      "Epoch 50000/100000, Loss: 0.4467\n",
      "Epoch 50100/100000, Loss: 0.4467\n",
      "Epoch 50200/100000, Loss: 0.4466\n",
      "Epoch 50300/100000, Loss: 0.4466\n",
      "Epoch 50400/100000, Loss: 0.4465\n",
      "Epoch 50500/100000, Loss: 0.4464\n",
      "Epoch 50600/100000, Loss: 0.4464\n",
      "Epoch 50700/100000, Loss: 0.4463\n",
      "Epoch 50800/100000, Loss: 0.4463\n",
      "Epoch 50900/100000, Loss: 0.4462\n",
      "Epoch 51000/100000, Loss: 0.4462\n",
      "Epoch 51100/100000, Loss: 0.4461\n",
      "Epoch 51200/100000, Loss: 0.4461\n",
      "Epoch 51300/100000, Loss: 0.4460\n",
      "Epoch 51400/100000, Loss: 0.4460\n",
      "Epoch 51500/100000, Loss: 0.4459\n",
      "Epoch 51600/100000, Loss: 0.4459\n",
      "Epoch 51700/100000, Loss: 0.4458\n",
      "Epoch 51800/100000, Loss: 0.4458\n",
      "Epoch 51900/100000, Loss: 0.4457\n",
      "Epoch 52000/100000, Loss: 0.4457\n",
      "Epoch 52100/100000, Loss: 0.4456\n",
      "Epoch 52200/100000, Loss: 0.4455\n",
      "Epoch 52300/100000, Loss: 0.4455\n",
      "Epoch 52400/100000, Loss: 0.4454\n",
      "Epoch 52500/100000, Loss: 0.4454\n",
      "Epoch 52600/100000, Loss: 0.4453\n",
      "Epoch 52700/100000, Loss: 0.4453\n",
      "Epoch 52800/100000, Loss: 0.4452\n",
      "Epoch 52900/100000, Loss: 0.4452\n",
      "Epoch 53000/100000, Loss: 0.4451\n",
      "Epoch 53100/100000, Loss: 0.4451\n",
      "Epoch 53200/100000, Loss: 0.4450\n",
      "Epoch 53300/100000, Loss: 0.4450\n",
      "Epoch 53400/100000, Loss: 0.4449\n",
      "Epoch 53500/100000, Loss: 0.4449\n",
      "Epoch 53600/100000, Loss: 0.4448\n",
      "Epoch 53700/100000, Loss: 0.4448\n",
      "Epoch 53800/100000, Loss: 0.4447\n",
      "Epoch 53900/100000, Loss: 0.4447\n",
      "Epoch 54000/100000, Loss: 0.4446\n",
      "Epoch 54100/100000, Loss: 0.4446\n",
      "Epoch 54200/100000, Loss: 0.4445\n",
      "Epoch 54300/100000, Loss: 0.4445\n",
      "Epoch 54400/100000, Loss: 0.4444\n",
      "Epoch 54500/100000, Loss: 0.4444\n",
      "Epoch 54600/100000, Loss: 0.4443\n",
      "Epoch 54700/100000, Loss: 0.4443\n",
      "Epoch 54800/100000, Loss: 0.4442\n",
      "Epoch 54900/100000, Loss: 0.4442\n",
      "Epoch 55000/100000, Loss: 0.4441\n",
      "Epoch 55100/100000, Loss: 0.4441\n",
      "Epoch 55200/100000, Loss: 0.4440\n",
      "Epoch 55300/100000, Loss: 0.4440\n",
      "Epoch 55400/100000, Loss: 0.4439\n",
      "Epoch 55500/100000, Loss: 0.4439\n",
      "Epoch 55600/100000, Loss: 0.4438\n",
      "Epoch 55700/100000, Loss: 0.4438\n",
      "Epoch 55800/100000, Loss: 0.4437\n",
      "Epoch 55900/100000, Loss: 0.4437\n",
      "Epoch 56000/100000, Loss: 0.4436\n",
      "Epoch 56100/100000, Loss: 0.4436\n",
      "Epoch 56200/100000, Loss: 0.4435\n",
      "Epoch 56300/100000, Loss: 0.4435\n",
      "Epoch 56400/100000, Loss: 0.4434\n",
      "Epoch 56500/100000, Loss: 0.4434\n",
      "Epoch 56600/100000, Loss: 0.4433\n",
      "Epoch 56700/100000, Loss: 0.4433\n",
      "Epoch 56800/100000, Loss: 0.4432\n",
      "Epoch 56900/100000, Loss: 0.4432\n",
      "Epoch 57000/100000, Loss: 0.4431\n",
      "Epoch 57100/100000, Loss: 0.4431\n",
      "Epoch 57200/100000, Loss: 0.4430\n",
      "Epoch 57300/100000, Loss: 0.4430\n",
      "Epoch 57400/100000, Loss: 0.4429\n",
      "Epoch 57500/100000, Loss: 0.4429\n",
      "Epoch 57600/100000, Loss: 0.4428\n",
      "Epoch 57700/100000, Loss: 0.4428\n",
      "Epoch 57800/100000, Loss: 0.4427\n",
      "Epoch 57900/100000, Loss: 0.4427\n",
      "Epoch 58000/100000, Loss: 0.4427\n",
      "Epoch 58100/100000, Loss: 0.4426\n",
      "Epoch 58200/100000, Loss: 0.4426\n",
      "Epoch 58300/100000, Loss: 0.4425\n",
      "Epoch 58400/100000, Loss: 0.4425\n",
      "Epoch 58500/100000, Loss: 0.4424\n",
      "Epoch 58600/100000, Loss: 0.4424\n",
      "Epoch 58700/100000, Loss: 0.4423\n",
      "Epoch 58800/100000, Loss: 0.4423\n",
      "Epoch 58900/100000, Loss: 0.4422\n",
      "Epoch 59000/100000, Loss: 0.4422\n",
      "Epoch 59100/100000, Loss: 0.4421\n",
      "Epoch 59200/100000, Loss: 0.4421\n",
      "Epoch 59300/100000, Loss: 0.4420\n",
      "Epoch 59400/100000, Loss: 0.4420\n",
      "Epoch 59500/100000, Loss: 0.4419\n",
      "Epoch 59600/100000, Loss: 0.4419\n",
      "Epoch 59700/100000, Loss: 0.4418\n",
      "Epoch 59800/100000, Loss: 0.4418\n",
      "Epoch 59900/100000, Loss: 0.4418\n",
      "Epoch 60000/100000, Loss: 0.4417\n",
      "Epoch 60100/100000, Loss: 0.4417\n",
      "Epoch 60200/100000, Loss: 0.4416\n",
      "Epoch 60300/100000, Loss: 0.4416\n",
      "Epoch 60400/100000, Loss: 0.4415\n",
      "Epoch 60500/100000, Loss: 0.4415\n",
      "Epoch 60600/100000, Loss: 0.4414\n",
      "Epoch 60700/100000, Loss: 0.4414\n",
      "Epoch 60800/100000, Loss: 0.4413\n",
      "Epoch 60900/100000, Loss: 0.4413\n",
      "Epoch 61000/100000, Loss: 0.4412\n",
      "Epoch 61100/100000, Loss: 0.4412\n",
      "Epoch 61200/100000, Loss: 0.4412\n",
      "Epoch 61300/100000, Loss: 0.4411\n",
      "Epoch 61400/100000, Loss: 0.4411\n",
      "Epoch 61500/100000, Loss: 0.4410\n",
      "Epoch 61600/100000, Loss: 0.4410\n",
      "Epoch 61700/100000, Loss: 0.4409\n",
      "Epoch 61800/100000, Loss: 0.4409\n",
      "Epoch 61900/100000, Loss: 0.4408\n",
      "Epoch 62000/100000, Loss: 0.4408\n",
      "Epoch 62100/100000, Loss: 0.4407\n",
      "Epoch 62200/100000, Loss: 0.4407\n",
      "Epoch 62300/100000, Loss: 0.4407\n",
      "Epoch 62400/100000, Loss: 0.4406\n",
      "Epoch 62500/100000, Loss: 0.4406\n",
      "Epoch 62600/100000, Loss: 0.4405\n",
      "Epoch 62700/100000, Loss: 0.4405\n",
      "Epoch 62800/100000, Loss: 0.4404\n",
      "Epoch 62900/100000, Loss: 0.4404\n",
      "Epoch 63000/100000, Loss: 0.4403\n",
      "Epoch 63100/100000, Loss: 0.4403\n",
      "Epoch 63200/100000, Loss: 0.4403\n",
      "Epoch 63300/100000, Loss: 0.4402\n",
      "Epoch 63400/100000, Loss: 0.4402\n",
      "Epoch 63500/100000, Loss: 0.4401\n",
      "Epoch 63600/100000, Loss: 0.4401\n",
      "Epoch 63700/100000, Loss: 0.4400\n",
      "Epoch 63800/100000, Loss: 0.4400\n",
      "Epoch 63900/100000, Loss: 0.4399\n",
      "Epoch 64000/100000, Loss: 0.4399\n",
      "Epoch 64100/100000, Loss: 0.4399\n",
      "Epoch 64200/100000, Loss: 0.4398\n",
      "Epoch 64300/100000, Loss: 0.4398\n",
      "Epoch 64400/100000, Loss: 0.4397\n",
      "Epoch 64500/100000, Loss: 0.4397\n",
      "Epoch 64600/100000, Loss: 0.4396\n",
      "Epoch 64700/100000, Loss: 0.4396\n",
      "Epoch 64800/100000, Loss: 0.4395\n",
      "Epoch 64900/100000, Loss: 0.4395\n",
      "Epoch 65000/100000, Loss: 0.4395\n",
      "Epoch 65100/100000, Loss: 0.4394\n",
      "Epoch 65200/100000, Loss: 0.4394\n",
      "Epoch 65300/100000, Loss: 0.4393\n",
      "Epoch 65400/100000, Loss: 0.4393\n",
      "Epoch 65500/100000, Loss: 0.4392\n",
      "Epoch 65600/100000, Loss: 0.4392\n",
      "Epoch 65700/100000, Loss: 0.4391\n",
      "Epoch 65800/100000, Loss: 0.4391\n",
      "Epoch 65900/100000, Loss: 0.4391\n",
      "Epoch 66000/100000, Loss: 0.4390\n",
      "Epoch 66100/100000, Loss: 0.4390\n",
      "Epoch 66200/100000, Loss: 0.4389\n",
      "Epoch 66300/100000, Loss: 0.4389\n",
      "Epoch 66400/100000, Loss: 0.4388\n",
      "Epoch 66500/100000, Loss: 0.4388\n",
      "Epoch 66600/100000, Loss: 0.4388\n",
      "Epoch 66700/100000, Loss: 0.4387\n",
      "Epoch 66800/100000, Loss: 0.4387\n",
      "Epoch 66900/100000, Loss: 0.4386\n",
      "Epoch 67000/100000, Loss: 0.4386\n",
      "Epoch 67100/100000, Loss: 0.4385\n",
      "Epoch 67200/100000, Loss: 0.4385\n",
      "Epoch 67300/100000, Loss: 0.4385\n",
      "Epoch 67400/100000, Loss: 0.4384\n",
      "Epoch 67500/100000, Loss: 0.4384\n",
      "Epoch 67600/100000, Loss: 0.4383\n",
      "Epoch 67700/100000, Loss: 0.4383\n",
      "Epoch 67800/100000, Loss: 0.4382\n",
      "Epoch 67900/100000, Loss: 0.4382\n",
      "Epoch 68000/100000, Loss: 0.4382\n",
      "Epoch 68100/100000, Loss: 0.4381\n",
      "Epoch 68200/100000, Loss: 0.4381\n",
      "Epoch 68300/100000, Loss: 0.4380\n",
      "Epoch 68400/100000, Loss: 0.4380\n",
      "Epoch 68500/100000, Loss: 0.4380\n",
      "Epoch 68600/100000, Loss: 0.4379\n",
      "Epoch 68700/100000, Loss: 0.4379\n",
      "Epoch 68800/100000, Loss: 0.4378\n",
      "Epoch 68900/100000, Loss: 0.4378\n",
      "Epoch 69000/100000, Loss: 0.4377\n",
      "Epoch 69100/100000, Loss: 0.4377\n",
      "Epoch 69200/100000, Loss: 0.4377\n",
      "Epoch 69300/100000, Loss: 0.4376\n",
      "Epoch 69400/100000, Loss: 0.4376\n",
      "Epoch 69500/100000, Loss: 0.4375\n",
      "Epoch 69600/100000, Loss: 0.4375\n",
      "Epoch 69700/100000, Loss: 0.4374\n",
      "Epoch 69800/100000, Loss: 0.4374\n",
      "Epoch 69900/100000, Loss: 0.4374\n",
      "Epoch 70000/100000, Loss: 0.4373\n",
      "Epoch 70100/100000, Loss: 0.4373\n",
      "Epoch 70200/100000, Loss: 0.4372\n",
      "Epoch 70300/100000, Loss: 0.4372\n",
      "Epoch 70400/100000, Loss: 0.4372\n",
      "Epoch 70500/100000, Loss: 0.4371\n",
      "Epoch 70600/100000, Loss: 0.4371\n",
      "Epoch 70700/100000, Loss: 0.4370\n",
      "Epoch 70800/100000, Loss: 0.4370\n",
      "Epoch 70900/100000, Loss: 0.4370\n",
      "Epoch 71000/100000, Loss: 0.4369\n",
      "Epoch 71100/100000, Loss: 0.4369\n",
      "Epoch 71200/100000, Loss: 0.4368\n",
      "Epoch 71300/100000, Loss: 0.4368\n",
      "Epoch 71400/100000, Loss: 0.4367\n",
      "Epoch 71500/100000, Loss: 0.4367\n",
      "Epoch 71600/100000, Loss: 0.4367\n",
      "Epoch 71700/100000, Loss: 0.4366\n",
      "Epoch 71800/100000, Loss: 0.4366\n",
      "Epoch 71900/100000, Loss: 0.4365\n",
      "Epoch 72000/100000, Loss: 0.4365\n",
      "Epoch 72100/100000, Loss: 0.4365\n",
      "Epoch 72200/100000, Loss: 0.4364\n",
      "Epoch 72300/100000, Loss: 0.4364\n",
      "Epoch 72400/100000, Loss: 0.4363\n",
      "Epoch 72500/100000, Loss: 0.4363\n",
      "Epoch 72600/100000, Loss: 0.4363\n",
      "Epoch 72700/100000, Loss: 0.4362\n",
      "Epoch 72800/100000, Loss: 0.4362\n",
      "Epoch 72900/100000, Loss: 0.4361\n",
      "Epoch 73000/100000, Loss: 0.4361\n",
      "Epoch 73100/100000, Loss: 0.4361\n",
      "Epoch 73200/100000, Loss: 0.4360\n",
      "Epoch 73300/100000, Loss: 0.4360\n",
      "Epoch 73400/100000, Loss: 0.4359\n",
      "Epoch 73500/100000, Loss: 0.4359\n",
      "Epoch 73600/100000, Loss: 0.4359\n",
      "Epoch 73700/100000, Loss: 0.4358\n",
      "Epoch 73800/100000, Loss: 0.4358\n",
      "Epoch 73900/100000, Loss: 0.4357\n",
      "Epoch 74000/100000, Loss: 0.4357\n",
      "Epoch 74100/100000, Loss: 0.4357\n",
      "Epoch 74200/100000, Loss: 0.4356\n",
      "Epoch 74300/100000, Loss: 0.4356\n",
      "Epoch 74400/100000, Loss: 0.4355\n",
      "Epoch 74500/100000, Loss: 0.4355\n",
      "Epoch 74600/100000, Loss: 0.4355\n",
      "Epoch 74700/100000, Loss: 0.4354\n",
      "Epoch 74800/100000, Loss: 0.4354\n",
      "Epoch 74900/100000, Loss: 0.4353\n",
      "Epoch 75000/100000, Loss: 0.4353\n",
      "Epoch 75100/100000, Loss: 0.4353\n",
      "Epoch 75200/100000, Loss: 0.4352\n",
      "Epoch 75300/100000, Loss: 0.4352\n",
      "Epoch 75400/100000, Loss: 0.4351\n",
      "Epoch 75500/100000, Loss: 0.4351\n",
      "Epoch 75600/100000, Loss: 0.4351\n",
      "Epoch 75700/100000, Loss: 0.4350\n",
      "Epoch 75800/100000, Loss: 0.4350\n",
      "Epoch 75900/100000, Loss: 0.4349\n",
      "Epoch 76000/100000, Loss: 0.4349\n",
      "Epoch 76100/100000, Loss: 0.4349\n",
      "Epoch 76200/100000, Loss: 0.4348\n",
      "Epoch 76300/100000, Loss: 0.4348\n",
      "Epoch 76400/100000, Loss: 0.4347\n",
      "Epoch 76500/100000, Loss: 0.4347\n",
      "Epoch 76600/100000, Loss: 0.4347\n",
      "Epoch 76700/100000, Loss: 0.4346\n",
      "Epoch 76800/100000, Loss: 0.4346\n",
      "Epoch 76900/100000, Loss: 0.4345\n",
      "Epoch 77000/100000, Loss: 0.4345\n",
      "Epoch 77100/100000, Loss: 0.4345\n",
      "Epoch 77200/100000, Loss: 0.4344\n",
      "Epoch 77300/100000, Loss: 0.4344\n",
      "Epoch 77400/100000, Loss: 0.4343\n",
      "Epoch 77500/100000, Loss: 0.4343\n",
      "Epoch 77600/100000, Loss: 0.4343\n",
      "Epoch 77700/100000, Loss: 0.4342\n",
      "Epoch 77800/100000, Loss: 0.4342\n",
      "Epoch 77900/100000, Loss: 0.4342\n",
      "Epoch 78000/100000, Loss: 0.4341\n",
      "Epoch 78100/100000, Loss: 0.4341\n",
      "Epoch 78200/100000, Loss: 0.4340\n",
      "Epoch 78300/100000, Loss: 0.4340\n",
      "Epoch 78400/100000, Loss: 0.4340\n",
      "Epoch 78500/100000, Loss: 0.4339\n",
      "Epoch 78600/100000, Loss: 0.4339\n",
      "Epoch 78700/100000, Loss: 0.4338\n",
      "Epoch 78800/100000, Loss: 0.4338\n",
      "Epoch 78900/100000, Loss: 0.4338\n",
      "Epoch 79000/100000, Loss: 0.4337\n",
      "Epoch 79100/100000, Loss: 0.4337\n",
      "Epoch 79200/100000, Loss: 0.4337\n",
      "Epoch 79300/100000, Loss: 0.4336\n",
      "Epoch 79400/100000, Loss: 0.4336\n",
      "Epoch 79500/100000, Loss: 0.4335\n",
      "Epoch 79600/100000, Loss: 0.4335\n",
      "Epoch 79700/100000, Loss: 0.4335\n",
      "Epoch 79800/100000, Loss: 0.4334\n",
      "Epoch 79900/100000, Loss: 0.4334\n",
      "Epoch 80000/100000, Loss: 0.4333\n",
      "Epoch 80100/100000, Loss: 0.4333\n",
      "Epoch 80200/100000, Loss: 0.4333\n",
      "Epoch 80300/100000, Loss: 0.4332\n",
      "Epoch 80400/100000, Loss: 0.4332\n",
      "Epoch 80500/100000, Loss: 0.4332\n",
      "Epoch 80600/100000, Loss: 0.4331\n",
      "Epoch 80700/100000, Loss: 0.4331\n",
      "Epoch 80800/100000, Loss: 0.4330\n",
      "Epoch 80900/100000, Loss: 0.4330\n",
      "Epoch 81000/100000, Loss: 0.4330\n",
      "Epoch 81100/100000, Loss: 0.4329\n",
      "Epoch 81200/100000, Loss: 0.4329\n",
      "Epoch 81300/100000, Loss: 0.4328\n",
      "Epoch 81400/100000, Loss: 0.4328\n",
      "Epoch 81500/100000, Loss: 0.4328\n",
      "Epoch 81600/100000, Loss: 0.4327\n",
      "Epoch 81700/100000, Loss: 0.4327\n",
      "Epoch 81800/100000, Loss: 0.4327\n",
      "Epoch 81900/100000, Loss: 0.4326\n",
      "Epoch 82000/100000, Loss: 0.4326\n",
      "Epoch 82100/100000, Loss: 0.4325\n",
      "Epoch 82200/100000, Loss: 0.4325\n",
      "Epoch 82300/100000, Loss: 0.4325\n",
      "Epoch 82400/100000, Loss: 0.4324\n",
      "Epoch 82500/100000, Loss: 0.4324\n",
      "Epoch 82600/100000, Loss: 0.4324\n",
      "Epoch 82700/100000, Loss: 0.4323\n",
      "Epoch 82800/100000, Loss: 0.4323\n",
      "Epoch 82900/100000, Loss: 0.4322\n",
      "Epoch 83000/100000, Loss: 0.4322\n",
      "Epoch 83100/100000, Loss: 0.4322\n",
      "Epoch 83200/100000, Loss: 0.4321\n",
      "Epoch 83300/100000, Loss: 0.4321\n",
      "Epoch 83400/100000, Loss: 0.4321\n",
      "Epoch 83500/100000, Loss: 0.4320\n",
      "Epoch 83600/100000, Loss: 0.4320\n",
      "Epoch 83700/100000, Loss: 0.4319\n",
      "Epoch 83800/100000, Loss: 0.4319\n",
      "Epoch 83900/100000, Loss: 0.4319\n",
      "Epoch 84000/100000, Loss: 0.4318\n",
      "Epoch 84100/100000, Loss: 0.4318\n",
      "Epoch 84200/100000, Loss: 0.4318\n",
      "Epoch 84300/100000, Loss: 0.4317\n",
      "Epoch 84400/100000, Loss: 0.4317\n",
      "Epoch 84500/100000, Loss: 0.4316\n",
      "Epoch 84600/100000, Loss: 0.4316\n",
      "Epoch 84700/100000, Loss: 0.4316\n",
      "Epoch 84800/100000, Loss: 0.4315\n",
      "Epoch 84900/100000, Loss: 0.4315\n",
      "Epoch 85000/100000, Loss: 0.4315\n",
      "Epoch 85100/100000, Loss: 0.4314\n",
      "Epoch 85200/100000, Loss: 0.4314\n",
      "Epoch 85300/100000, Loss: 0.4313\n",
      "Epoch 85400/100000, Loss: 0.4313\n",
      "Epoch 85500/100000, Loss: 0.4313\n",
      "Epoch 85600/100000, Loss: 0.4312\n",
      "Epoch 85700/100000, Loss: 0.4312\n",
      "Epoch 85800/100000, Loss: 0.4312\n",
      "Epoch 85900/100000, Loss: 0.4311\n",
      "Epoch 86000/100000, Loss: 0.4311\n",
      "Epoch 86100/100000, Loss: 0.4310\n",
      "Epoch 86200/100000, Loss: 0.4310\n",
      "Epoch 86300/100000, Loss: 0.4310\n",
      "Epoch 86400/100000, Loss: 0.4309\n",
      "Epoch 86500/100000, Loss: 0.4309\n",
      "Epoch 86600/100000, Loss: 0.4309\n",
      "Epoch 86700/100000, Loss: 0.4308\n",
      "Epoch 86800/100000, Loss: 0.4308\n",
      "Epoch 86900/100000, Loss: 0.4308\n",
      "Epoch 87000/100000, Loss: 0.4307\n",
      "Epoch 87100/100000, Loss: 0.4307\n",
      "Epoch 87200/100000, Loss: 0.4306\n",
      "Epoch 87300/100000, Loss: 0.4306\n",
      "Epoch 87400/100000, Loss: 0.4306\n",
      "Epoch 87500/100000, Loss: 0.4305\n",
      "Epoch 87600/100000, Loss: 0.4305\n",
      "Epoch 87700/100000, Loss: 0.4305\n",
      "Epoch 87800/100000, Loss: 0.4304\n",
      "Epoch 87900/100000, Loss: 0.4304\n",
      "Epoch 88000/100000, Loss: 0.4303\n",
      "Epoch 88100/100000, Loss: 0.4303\n",
      "Epoch 88200/100000, Loss: 0.4303\n",
      "Epoch 88300/100000, Loss: 0.4302\n",
      "Epoch 88400/100000, Loss: 0.4302\n",
      "Epoch 88500/100000, Loss: 0.4302\n",
      "Epoch 88600/100000, Loss: 0.4301\n",
      "Epoch 88700/100000, Loss: 0.4301\n",
      "Epoch 88800/100000, Loss: 0.4301\n",
      "Epoch 88900/100000, Loss: 0.4300\n",
      "Epoch 89000/100000, Loss: 0.4300\n",
      "Epoch 89100/100000, Loss: 0.4299\n",
      "Epoch 89200/100000, Loss: 0.4299\n",
      "Epoch 89300/100000, Loss: 0.4299\n",
      "Epoch 89400/100000, Loss: 0.4298\n",
      "Epoch 89500/100000, Loss: 0.4298\n",
      "Epoch 89600/100000, Loss: 0.4298\n",
      "Epoch 89700/100000, Loss: 0.4297\n",
      "Epoch 89800/100000, Loss: 0.4297\n",
      "Epoch 89900/100000, Loss: 0.4297\n",
      "Epoch 90000/100000, Loss: 0.4296\n",
      "Epoch 90100/100000, Loss: 0.4296\n",
      "Epoch 90200/100000, Loss: 0.4295\n",
      "Epoch 90300/100000, Loss: 0.4295\n",
      "Epoch 90400/100000, Loss: 0.4295\n",
      "Epoch 90500/100000, Loss: 0.4294\n",
      "Epoch 90600/100000, Loss: 0.4294\n",
      "Epoch 90700/100000, Loss: 0.4294\n",
      "Epoch 90800/100000, Loss: 0.4293\n",
      "Epoch 90900/100000, Loss: 0.4293\n",
      "Epoch 91000/100000, Loss: 0.4293\n",
      "Epoch 91100/100000, Loss: 0.4292\n",
      "Epoch 91200/100000, Loss: 0.4292\n",
      "Epoch 91300/100000, Loss: 0.4291\n",
      "Epoch 91400/100000, Loss: 0.4291\n",
      "Epoch 91500/100000, Loss: 0.4291\n",
      "Epoch 91600/100000, Loss: 0.4290\n",
      "Epoch 91700/100000, Loss: 0.4290\n",
      "Epoch 91800/100000, Loss: 0.4290\n",
      "Epoch 91900/100000, Loss: 0.4289\n",
      "Epoch 92000/100000, Loss: 0.4289\n",
      "Epoch 92100/100000, Loss: 0.4289\n",
      "Epoch 92200/100000, Loss: 0.4288\n",
      "Epoch 92300/100000, Loss: 0.4288\n",
      "Epoch 92400/100000, Loss: 0.4288\n",
      "Epoch 92500/100000, Loss: 0.4287\n",
      "Epoch 92600/100000, Loss: 0.4287\n",
      "Epoch 92700/100000, Loss: 0.4286\n",
      "Epoch 92800/100000, Loss: 0.4286\n",
      "Epoch 92900/100000, Loss: 0.4286\n",
      "Epoch 93000/100000, Loss: 0.4285\n",
      "Epoch 93100/100000, Loss: 0.4285\n",
      "Epoch 93200/100000, Loss: 0.4285\n",
      "Epoch 93300/100000, Loss: 0.4284\n",
      "Epoch 93400/100000, Loss: 0.4284\n",
      "Epoch 93500/100000, Loss: 0.4284\n",
      "Epoch 93600/100000, Loss: 0.4283\n",
      "Epoch 93700/100000, Loss: 0.4283\n",
      "Epoch 93800/100000, Loss: 0.4282\n",
      "Epoch 93900/100000, Loss: 0.4282\n",
      "Epoch 94000/100000, Loss: 0.4282\n",
      "Epoch 94100/100000, Loss: 0.4281\n",
      "Epoch 94200/100000, Loss: 0.4281\n",
      "Epoch 94300/100000, Loss: 0.4281\n",
      "Epoch 94400/100000, Loss: 0.4280\n",
      "Epoch 94500/100000, Loss: 0.4280\n",
      "Epoch 94600/100000, Loss: 0.4280\n",
      "Epoch 94700/100000, Loss: 0.4279\n",
      "Epoch 94800/100000, Loss: 0.4279\n",
      "Epoch 94900/100000, Loss: 0.4279\n",
      "Epoch 95000/100000, Loss: 0.4278\n",
      "Epoch 95100/100000, Loss: 0.4278\n",
      "Epoch 95200/100000, Loss: 0.4277\n",
      "Epoch 95300/100000, Loss: 0.4277\n",
      "Epoch 95400/100000, Loss: 0.4277\n",
      "Epoch 95500/100000, Loss: 0.4276\n",
      "Epoch 95600/100000, Loss: 0.4276\n",
      "Epoch 95700/100000, Loss: 0.4276\n",
      "Epoch 95800/100000, Loss: 0.4275\n",
      "Epoch 95900/100000, Loss: 0.4275\n",
      "Epoch 96000/100000, Loss: 0.4275\n",
      "Epoch 96100/100000, Loss: 0.4274\n",
      "Epoch 96200/100000, Loss: 0.4274\n",
      "Epoch 96300/100000, Loss: 0.4274\n",
      "Epoch 96400/100000, Loss: 0.4273\n",
      "Epoch 96500/100000, Loss: 0.4273\n",
      "Epoch 96600/100000, Loss: 0.4272\n",
      "Epoch 96700/100000, Loss: 0.4272\n",
      "Epoch 96800/100000, Loss: 0.4272\n",
      "Epoch 96900/100000, Loss: 0.4271\n",
      "Epoch 97000/100000, Loss: 0.4271\n",
      "Epoch 97100/100000, Loss: 0.4271\n",
      "Epoch 97200/100000, Loss: 0.4270\n",
      "Epoch 97300/100000, Loss: 0.4270\n",
      "Epoch 97400/100000, Loss: 0.4270\n",
      "Epoch 97500/100000, Loss: 0.4269\n",
      "Epoch 97600/100000, Loss: 0.4269\n",
      "Epoch 97700/100000, Loss: 0.4269\n",
      "Epoch 97800/100000, Loss: 0.4268\n",
      "Epoch 97900/100000, Loss: 0.4268\n",
      "Epoch 98000/100000, Loss: 0.4268\n",
      "Epoch 98100/100000, Loss: 0.4267\n",
      "Epoch 98200/100000, Loss: 0.4267\n",
      "Epoch 98300/100000, Loss: 0.4266\n",
      "Epoch 98400/100000, Loss: 0.4266\n",
      "Epoch 98500/100000, Loss: 0.4266\n",
      "Epoch 98600/100000, Loss: 0.4265\n",
      "Epoch 98700/100000, Loss: 0.4265\n",
      "Epoch 98800/100000, Loss: 0.4265\n",
      "Epoch 98900/100000, Loss: 0.4264\n",
      "Epoch 99000/100000, Loss: 0.4264\n",
      "Epoch 99100/100000, Loss: 0.4264\n",
      "Epoch 99200/100000, Loss: 0.4263\n",
      "Epoch 99300/100000, Loss: 0.4263\n",
      "Epoch 99400/100000, Loss: 0.4263\n",
      "Epoch 99500/100000, Loss: 0.4262\n",
      "Epoch 99600/100000, Loss: 0.4262\n",
      "Epoch 99700/100000, Loss: 0.4261\n",
      "Epoch 99800/100000, Loss: 0.4261\n",
      "Epoch 99900/100000, Loss: 0.4261\n",
      "\n",
      "Avaliação do MLP:\n",
      "Acurácia: 0.6440677966101694\n",
      "Precisão: 0.4666666666666667\n",
      "Revocação (Recall): 0.35\n",
      "F1-score: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Inicializando e treinando o MLP\n",
    "mlp = MLP(input_dim=X_train_scaled.shape[1], hidden_dim=8, output_dim=1, learning_rate=0.01)\n",
    "\n",
    "# Treinamento (usando 100000 épocas) - convertendo Series para array NumPy\n",
    "mlp.train(X_train_scaled, y_train.values, epochs=100000)\n",
    "\n",
    "# Previsões com o MLP\n",
    "y_pred_mlp_proba = mlp.predict(X_test_scaled)\n",
    "y_pred_mlp = mlp.predict_classes(X_test_scaled)\n",
    "\n",
    "# Flatten para garantir dimensões corretas\n",
    "y_pred_mlp = y_pred_mlp.flatten()\n",
    "\n",
    "# Avaliação do MLP\n",
    "print(\"\\nAvaliação do MLP:\")\n",
    "print(\"Acurácia:\", accuracy_score(y_val, y_pred_mlp))\n",
    "print(\"Precisão:\", precision_score(y_val, y_pred_mlp))\n",
    "print(\"Revocação (Recall):\", recall_score(y_val, y_pred_mlp))\n",
    "print(\"F1-score:\", f1_score(y_val, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4027cc3",
   "metadata": {},
   "source": [
    "### Comparação RL e MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7f5aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparação dos Modelos:\n",
      "                 Model  Accuracy  Precision  Recall  F1-Score   Time (s)\n",
      "0  Logistic Regression  0.813559   0.736842    0.70  0.717949   0.018258\n",
      "1                  MLP  0.779661   0.684211    0.65  0.666667  10.089914\n"
     ]
    }
   ],
   "source": [
    "# Primeira parte - Regressão Logística\n",
    "start_time = time.time()\n",
    "logreg = LogisticRegression(max_iter=100000, random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_logreg = logreg.predict(X_val_scaled)\n",
    "logreg_time = time.time() - start_time\n",
    "\n",
    "# Avaliação da regressão logística\n",
    "logreg_accuracy = accuracy_score(y_val, y_pred_logreg)\n",
    "logreg_precision = precision_score(y_val, y_pred_logreg, zero_division=0)\n",
    "logreg_recall = recall_score(y_val, y_pred_logreg, zero_division=0)\n",
    "logreg_f1 = f1_score(y_val, y_pred_logreg, zero_division=0)\n",
    "\n",
    "# Segunda parte - MLP\n",
    "start_time = time.time()\n",
    "mlp_comparison = MLP(input_dim=X_train_scaled.shape[1], hidden_dim=8, output_dim=1, \n",
    "                    learning_rate=0.01, random_state=42)\n",
    "mlp_comparison.train(X_train_scaled, y_train.values, epochs=100000, verbose=False)\n",
    "y_pred_mlp_comparison = mlp_comparison.predict_classes(X_val_scaled).flatten()\n",
    "mlp_time = time.time() - start_time\n",
    "\n",
    "# Avaliação do MLP\n",
    "mlp_accuracy = accuracy_score(y_val, y_pred_mlp_comparison)\n",
    "mlp_precision = precision_score(y_val, y_pred_mlp_comparison, zero_division=0)\n",
    "mlp_recall = recall_score(y_val, y_pred_mlp_comparison, zero_division=0)\n",
    "mlp_f1 = f1_score(y_val, y_pred_mlp_comparison, zero_division=0)\n",
    "\n",
    "# Exibindo os resultados\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'MLP'],\n",
    "    'Accuracy': [logreg_accuracy, mlp_accuracy],\n",
    "    'Precision': [logreg_precision, mlp_precision],\n",
    "    'Recall': [logreg_recall, mlp_recall],\n",
    "    'F1-Score': [logreg_f1, mlp_f1],\n",
    "    'Time (s)': [logreg_time, mlp_time]\n",
    "})\n",
    "\n",
    "print(\"\\nComparação dos Modelos:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ba40c",
   "metadata": {},
   "source": [
    "### Análise da complexidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155c88e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise de Complexidade Computacional:\n",
      "==================================================\n",
      "Logistic Regression - Time: 0.0046s, Memory: 0.0000MB\n",
      "MLP - Time: 9.6432s, Memory: 3.6016MB\n",
      "\n",
      "Resumo da Comparação:\n",
      "Speedup (MLP vs LogReg): 2115.52x mais lento\n",
      "Memory overhead (MLP vs LogReg): 3601.56x mais memória\n"
     ]
    }
   ],
   "source": [
    "def measure_complexity(model_func, X, y, model_name):\n",
    "    \"\"\"\n",
    "    Função para medir o uso de memória e tempo de treinamento\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Memória inicial (MB)\n",
    "    initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "    \n",
    "    # Executa o treinamento\n",
    "    model = model_func(X, y)\n",
    "    \n",
    "    # Tempo de execução e memória final (MB)\n",
    "    end_time = time.time()\n",
    "    final_memory = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    memory_usage = final_memory - initial_memory\n",
    "    \n",
    "    print(f\"{model_name} - Time: {execution_time:.4f}s, Memory: {memory_usage:.4f}MB\")\n",
    "    \n",
    "    return execution_time, memory_usage, model\n",
    "\n",
    "def train_logistic_regression(X, y):\n",
    "    \"\"\"Função para treinar regressão logística\"\"\"\n",
    "    model = LogisticRegression(max_iter=100000, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def train_mlp(X, y):\n",
    "    \"\"\"Função para treinar MLP\"\"\"\n",
    "    model = MLP(input_dim=X.shape[1], hidden_dim=8, output_dim=1, \n",
    "               learning_rate=0.01, random_state=42)\n",
    "    model.train(X, y, epochs=100000, verbose=False)\n",
    "    return model\n",
    "\n",
    "# Comparando a complexidade computacional dos modelos\n",
    "print(\"Análise de Complexidade Computacional:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "logreg_time, logreg_memory, logreg_model = measure_complexity(\n",
    "    train_logistic_regression, X_train_scaled, y_train, \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "mlp_time, mlp_memory, mlp_model = measure_complexity(\n",
    "    train_mlp, X_train_scaled, y_train.values, \"MLP\"  # Convertendo para array NumPy\n",
    ")\n",
    "\n",
    "# Resumo da comparação\n",
    "print(\"\\nResumo da Comparação:\")\n",
    "print(f\"Speedup (MLP vs LogReg): {mlp_time/logreg_time:.2f}x mais lento\")\n",
    "print(f\"Memory overhead (MLP vs LogReg): {mlp_memory/max(logreg_memory, 0.001):.2f}x mais memória\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
